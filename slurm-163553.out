[I 2025-10-16 22:29:19,000] A new study created in RDB with name: Mamba_Exchange_1

======================================================================
Starting Optuna Hyperparameter Optimization
======================================================================
Model: Mamba
Dataset: custom
Number of trials: 50
Study name: Mamba_Exchange_1
Storage: sqlite:///optuna_study.db
======================================================================

  0%|          | 0/50 [00:00<?, ?it/s]
============================================================
Trial 0
============================================================
seq_len: 96, pred_len: 7
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 1
batch_size: 16, lr: 0.000015
dropout: 0.06727091761431199
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_0 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.036831).  Saving model ...
Updating learning rate to 1.4810698749927847e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.405349374963923e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.7026746874819617e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_0 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06723527610301971, mae:0.11485341936349869, dtw:Not calculated


                                      [I 2025-10-16 22:29:32,247] Trial 0 finished with value: 0.11485341936349869 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 1.4810698749927847e-05, 'dropout': 0.06727091761431199}. Best is trial 0 with value: 0.11485341936349869.
  0%|          | 0/50 [00:13<?, ?it/s]Best trial: 0. Best value: 0.114853:   0%|          | 0/50 [00:13<?, ?it/s]Best trial: 0. Best value: 0.114853:   2%|▏         | 1/50 [00:13<10:49, 13.26s/it]
============================================================
Trial 1
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000019
dropout: 0.17354827937911066
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_1 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.039065).  Saving model ...
Updating learning rate to 1.862445116966383e-05
Validation loss decreased (0.039065 --> 0.038511).  Saving model ...
Updating learning rate to 9.312225584831915e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.6561127924159576e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3280563962079788e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_1 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06429198384284973, mae:0.09888728708028793, dtw:Not calculated


                                                                                   [I 2025-10-16 22:29:45,961] Trial 1 finished with value: 0.09888728708028793 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 1.862445116966383e-05, 'dropout': 0.17354827937911066}. Best is trial 1 with value: 0.09888728708028793.
Best trial: 0. Best value: 0.114853:   2%|▏         | 1/50 [00:26<10:49, 13.26s/it]Best trial: 1. Best value: 0.0988873:   2%|▏         | 1/50 [00:26<10:49, 13.26s/it]Best trial: 1. Best value: 0.0988873:   4%|▍         | 2/50 [00:26<10:49, 13.53s/it]
============================================================
Trial 2
============================================================
seq_len: 96, pred_len: 14
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000173
dropout: 0.1116964351617344
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_2 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.039771).  Saving model ...
Updating learning rate to 0.000172891076001466
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.6445538000733e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.32227690003665e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_2 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06915130466222763, mae:0.11119924485683441, dtw:Not calculated


                                                                                    [I 2025-10-16 22:29:51,750] Trial 2 finished with value: 0.11119924485683441 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.000172891076001466, 'dropout': 0.1116964351617344}. Best is trial 1 with value: 0.09888728708028793.
Best trial: 1. Best value: 0.0988873:   4%|▍         | 2/50 [00:32<10:49, 13.53s/it]Best trial: 1. Best value: 0.0988873:   4%|▍         | 2/50 [00:32<10:49, 13.53s/it]Best trial: 1. Best value: 0.0988873:   6%|▌         | 3/50 [00:32<07:49,  9.99s/it]
============================================================
Trial 3
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 1
batch_size: 64, lr: 0.000096
dropout: 0.18966312607988225
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_3 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.037678).  Saving model ...
Updating learning rate to 9.626015714350737e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.8130078571753685e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.4065039285876843e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_3 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06382521241903305, mae:0.0971994400024414, dtw:Not calculated


                                                                                    [I 2025-10-16 22:29:55,044] Trial 3 finished with value: 0.0971994400024414 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 9.626015714350737e-05, 'dropout': 0.18966312607988225}. Best is trial 3 with value: 0.0971994400024414.
Best trial: 1. Best value: 0.0988873:   6%|▌         | 3/50 [00:36<07:49,  9.99s/it]Best trial: 3. Best value: 0.0971994:   6%|▌         | 3/50 [00:36<07:49,  9.99s/it]Best trial: 3. Best value: 0.0971994:   8%|▊         | 4/50 [00:36<05:38,  7.35s/it]
============================================================
Trial 4
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000018
dropout: 0.29906298438403434
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_4 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.031057).  Saving model ...
Updating learning rate to 1.761835683234762e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.80917841617381e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.404589208086905e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_4 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.06131169572472572, mae:0.09406031668186188, dtw:Not calculated


                                                                                    [I 2025-10-16 22:30:00,797] Trial 4 finished with value: 0.09406031668186188 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 1.761835683234762e-05, 'dropout': 0.29906298438403434}. Best is trial 4 with value: 0.09406031668186188.
Best trial: 3. Best value: 0.0971994:   8%|▊         | 4/50 [00:41<05:38,  7.35s/it]Best trial: 4. Best value: 0.0940603:   8%|▊         | 4/50 [00:41<05:38,  7.35s/it]Best trial: 4. Best value: 0.0940603:  10%|█         | 5/50 [00:41<05:04,  6.77s/it]
============================================================
Trial 5
============================================================
seq_len: 96, pred_len: 30
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000301
dropout: 0.19714544839967285
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_5 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.034503).  Saving model ...
Updating learning rate to 0.0003008625552395355
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015043127761976775
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.521563880988387e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_5 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07243028283119202, mae:0.11405383050441742, dtw:Not calculated


                                                                                    [I 2025-10-16 22:30:06,562] Trial 5 finished with value: 0.11405383050441742 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0003008625552395355, 'dropout': 0.19714544839967285}. Best is trial 4 with value: 0.09406031668186188.
Best trial: 4. Best value: 0.0940603:  10%|█         | 5/50 [00:47<05:04,  6.77s/it]Best trial: 4. Best value: 0.0940603:  10%|█         | 5/50 [00:47<05:04,  6.77s/it]Best trial: 4. Best value: 0.0940603:  12%|█▏        | 6/50 [00:47<04:42,  6.43s/it]
============================================================
Trial 6
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000036
dropout: 0.031086378108234336
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_6 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.037488).  Saving model ...
Updating learning rate to 3.644184789734065e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.8220923948670324e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.110461974335162e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_6 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.0646069124341011, mae:0.09841065853834152, dtw:Not calculated


                                                                                    [I 2025-10-16 22:30:12,232] Trial 6 finished with value: 0.09841065853834152 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 3.644184789734065e-05, 'dropout': 0.031086378108234336}. Best is trial 4 with value: 0.09406031668186188.
Best trial: 4. Best value: 0.0940603:  12%|█▏        | 6/50 [00:53<04:42,  6.43s/it]Best trial: 4. Best value: 0.0940603:  12%|█▏        | 6/50 [00:53<04:42,  6.43s/it]Best trial: 4. Best value: 0.0940603:  14%|█▍        | 7/50 [00:53<04:25,  6.18s/it]
============================================================
Trial 7
============================================================
seq_len: 96, pred_len: 14
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 64, lr: 0.000043
dropout: 0.02221415167345244
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_7 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.036214).  Saving model ...
Updating learning rate to 4.288771132984285e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.1443855664921426e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.0721927832460713e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_7 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06790991127490997, mae:0.11625023931264877, dtw:Not calculated


                                                                                    [I 2025-10-16 22:30:15,325] Trial 7 finished with value: 0.11625023931264877 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 1, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 4.288771132984285e-05, 'dropout': 0.02221415167345244}. Best is trial 4 with value: 0.09406031668186188.
Best trial: 4. Best value: 0.0940603:  14%|█▍        | 7/50 [00:56<04:25,  6.18s/it]Best trial: 4. Best value: 0.0940603:  14%|█▍        | 7/50 [00:56<04:25,  6.18s/it]Best trial: 4. Best value: 0.0940603:  16%|█▌        | 8/50 [00:56<03:38,  5.20s/it]
============================================================
Trial 8
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000145
dropout: 0.048108435488234055
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_8 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.031012).  Saving model ...
Updating learning rate to 0.00014546317465644103
Validation loss decreased (0.031012 --> 0.027837).  Saving model ...
Updating learning rate to 7.273158732822051e-05
Validation loss decreased (0.027837 --> 0.027383).  Saving model ...
Updating learning rate to 3.636579366411026e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.818289683205513e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.091448416027564e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_8 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05227626487612724, mae:0.08076974004507065, dtw:Not calculated


                                                                                    [I 2025-10-16 22:30:31,654] Trial 8 finished with value: 0.08076974004507065 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00014546317465644103, 'dropout': 0.048108435488234055}. Best is trial 8 with value: 0.08076974004507065.
Best trial: 4. Best value: 0.0940603:  16%|█▌        | 8/50 [01:12<03:38,  5.20s/it]Best trial: 8. Best value: 0.0807697:  16%|█▌        | 8/50 [01:12<03:38,  5.20s/it]Best trial: 8. Best value: 0.0807697:  18%|█▊        | 9/50 [01:12<05:55,  8.68s/it]
============================================================
Trial 9
============================================================
seq_len: 96, pred_len: 7
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 64, lr: 0.000013
dropout: 0.03975384925661524
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_9 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.036962).  Saving model ...
Updating learning rate to 1.2898867314935148e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.449433657467574e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.224716828733787e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_9 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06710508465766907, mae:0.11558031290769577, dtw:Not calculated


                                                                                    [I 2025-10-16 22:30:35,082] Trial 9 finished with value: 0.11558031290769577 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 1.2898867314935148e-05, 'dropout': 0.03975384925661524}. Best is trial 8 with value: 0.08076974004507065.
Best trial: 8. Best value: 0.0807697:  18%|█▊        | 9/50 [01:16<05:55,  8.68s/it]Best trial: 8. Best value: 0.0807697:  18%|█▊        | 9/50 [01:16<05:55,  8.68s/it]Best trial: 8. Best value: 0.0807697:  20%|██        | 10/50 [01:16<04:42,  7.06s/it]
============================================================
Trial 10
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000699
dropout: 0.11271043672094821
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_10 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026954).  Saving model ...
Updating learning rate to 0.0006986097758772165
Validation loss decreased (0.026954 --> 0.026290).  Saving model ...
Updating learning rate to 0.00034930488793860826
Validation loss decreased (0.026290 --> 0.025029).  Saving model ...
Updating learning rate to 0.00017465244396930413
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.732622198465207e-05
Validation loss decreased (0.025029 --> 0.024732).  Saving model ...
Updating learning rate to 4.366311099232603e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.1831555496163016e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.0915777748081508e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_10 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.048614367842674255, mae:0.0712396502494812, dtw:Not calculated


                                                                                     [I 2025-10-16 22:30:56,305] Trial 10 finished with value: 0.0712396502494812 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0006986097758772165, 'dropout': 0.11271043672094821}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 8. Best value: 0.0807697:  20%|██        | 10/50 [01:37<04:42,  7.06s/it]Best trial: 10. Best value: 0.0712397:  20%|██        | 10/50 [01:37<04:42,  7.06s/it]Best trial: 10. Best value: 0.0712397:  22%|██▏       | 11/50 [01:37<07:24, 11.39s/it]
============================================================
Trial 11
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000892
dropout: 0.11033612836332557
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_11 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026420).  Saving model ...
Updating learning rate to 0.00089206705117009
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000446033525585045
Validation loss decreased (0.026420 --> 0.025175).  Saving model ...
Updating learning rate to 0.0002230167627925225
Validation loss decreased (0.025175 --> 0.024671).  Saving model ...
Updating learning rate to 0.00011150838139626126
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.575419069813063e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.7877095349065314e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_11 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05129503086209297, mae:0.07293318212032318, dtw:Not calculated


                                                                                      [I 2025-10-16 22:31:15,019] Trial 11 finished with value: 0.07293318212032318 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00089206705117009, 'dropout': 0.11033612836332557}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  22%|██▏       | 11/50 [01:56<07:24, 11.39s/it]Best trial: 10. Best value: 0.0712397:  22%|██▏       | 11/50 [01:56<07:24, 11.39s/it]Best trial: 10. Best value: 0.0712397:  24%|██▍       | 12/50 [01:56<08:37, 13.62s/it]
============================================================
Trial 12
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000949
dropout: 0.12530168629040936
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_12 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.024565).  Saving model ...
Updating learning rate to 0.0009494845925790892
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004747422962895446
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002373711481447723
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_12 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.048945922404527664, mae:0.07403426617383957, dtw:Not calculated


                                                                                      [I 2025-10-16 22:31:25,845] Trial 12 finished with value: 0.07403426617383957 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009494845925790892, 'dropout': 0.12530168629040936}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  24%|██▍       | 12/50 [02:06<08:37, 13.62s/it]Best trial: 10. Best value: 0.0712397:  24%|██▍       | 12/50 [02:06<08:37, 13.62s/it]Best trial: 10. Best value: 0.0712397:  26%|██▌       | 13/50 [02:06<07:52, 12.77s/it]
============================================================
Trial 13
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000948
dropout: 0.09575395982914629
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_13 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.027682).  Saving model ...
Updating learning rate to 0.0009484005756964168
Validation loss decreased (0.027682 --> 0.026171).  Saving model ...
Updating learning rate to 0.0004742002878482084
Validation loss decreased (0.026171 --> 0.026022).  Saving model ...
Updating learning rate to 0.0002371001439241042
Validation loss decreased (0.026022 --> 0.024990).  Saving model ...
Updating learning rate to 0.0001185500719620521
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.927503598102605e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.9637517990513026e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_13 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04902718961238861, mae:0.07162290811538696, dtw:Not calculated


                                                                                      [I 2025-10-16 22:31:44,550] Trial 13 finished with value: 0.07162290811538696 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009484005756964168, 'dropout': 0.09575395982914629}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  26%|██▌       | 13/50 [02:25<07:52, 12.77s/it]Best trial: 10. Best value: 0.0712397:  26%|██▌       | 13/50 [02:25<07:52, 12.77s/it]Best trial: 10. Best value: 0.0712397:  28%|██▊       | 14/50 [02:25<08:44, 14.56s/it]
============================================================
Trial 14
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000497
dropout: 0.24607416960194545
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_14 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.030085).  Saving model ...
Updating learning rate to 0.0004972150464381177
Validation loss decreased (0.030085 --> 0.027698).  Saving model ...
Updating learning rate to 0.00024860752321905887
Validation loss decreased (0.027698 --> 0.026249).  Saving model ...
Updating learning rate to 0.00012430376160952944
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.215188080476472e-05
Validation loss decreased (0.026249 --> 0.026026).  Saving model ...
Updating learning rate to 3.107594040238236e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.553797020119118e-05
Validation loss decreased (0.026026 --> 0.025256).  Saving model ...
Updating learning rate to 7.76898510059559e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.884492550297795e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.9422462751488974e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_14 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05009645223617554, mae:0.07580137252807617, dtw:Not calculated


                                                                                      [I 2025-10-16 22:32:11,078] Trial 14 finished with value: 0.07580137252807617 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004972150464381177, 'dropout': 0.24607416960194545}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  28%|██▊       | 14/50 [02:52<08:44, 14.56s/it]Best trial: 10. Best value: 0.0712397:  28%|██▊       | 14/50 [02:52<08:44, 14.56s/it]Best trial: 10. Best value: 0.0712397:  30%|███       | 15/50 [02:52<10:35, 18.17s/it]
============================================================
Trial 15
============================================================
seq_len: 48, pred_len: 30
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000478
dropout: 0.08235384111285679
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_15 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037669).  Saving model ...
Updating learning rate to 0.0004784306628671783
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00023921533143358916
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011960766571679458
Validation loss decreased (0.037669 --> 0.037582).  Saving model ...
Updating learning rate to 5.980383285839729e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.9901916429198645e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.4950958214599322e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_15 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07656216621398926, mae:0.11230042576789856, dtw:Not calculated


                                                                                      [I 2025-10-16 22:32:29,591] Trial 15 finished with value: 0.11230042576789856 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004784306628671783, 'dropout': 0.08235384111285679}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  30%|███       | 15/50 [03:10<10:35, 18.17s/it]Best trial: 10. Best value: 0.0712397:  30%|███       | 15/50 [03:10<10:35, 18.17s/it]Best trial: 10. Best value: 0.0712397:  32%|███▏      | 16/50 [03:10<10:21, 18.27s/it]
============================================================
Trial 16
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000469
dropout: 0.14793220522310344
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_16 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.034508).  Saving model ...
Updating learning rate to 0.0004689010078077457
Validation loss decreased (0.034508 --> 0.029365).  Saving model ...
Updating learning rate to 0.00023445050390387285
Validation loss decreased (0.029365 --> 0.026399).  Saving model ...
Updating learning rate to 0.00011722525195193642
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.861262597596821e-05
Validation loss decreased (0.026399 --> 0.025123).  Saving model ...
Updating learning rate to 2.9306312987984106e-05
Validation loss decreased (0.025123 --> 0.024789).  Saving model ...
Updating learning rate to 1.4653156493992053e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.326578246996026e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.663289123498013e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_16 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04885703697800636, mae:0.07222942262887955, dtw:Not calculated


                                                                                      [I 2025-10-16 22:32:53,564] Trial 16 finished with value: 0.07222942262887955 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004689010078077457, 'dropout': 0.14793220522310344}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  32%|███▏      | 16/50 [03:34<10:21, 18.27s/it]Best trial: 10. Best value: 0.0712397:  32%|███▏      | 16/50 [03:34<10:21, 18.27s/it]Best trial: 10. Best value: 0.0712397:  34%|███▍      | 17/50 [03:34<10:59, 19.99s/it]
============================================================
Trial 17
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000239
dropout: 0.09044511278381279
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_17 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.031395).  Saving model ...
Updating learning rate to 0.0002391488004471301
Validation loss decreased (0.031395 --> 0.026602).  Saving model ...
Updating learning rate to 0.00011957440022356505
Validation loss decreased (0.026602 --> 0.026143).  Saving model ...
Updating learning rate to 5.978720011178253e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.9893600055891264e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.4946800027945632e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_17 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05153516307473183, mae:0.07697153836488724, dtw:Not calculated


                                                                                      [I 2025-10-16 22:33:09,738] Trial 17 finished with value: 0.07697153836488724 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0002391488004471301, 'dropout': 0.09044511278381279}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  34%|███▍      | 17/50 [03:50<10:59, 19.99s/it]Best trial: 10. Best value: 0.0712397:  34%|███▍      | 17/50 [03:50<10:59, 19.99s/it]Best trial: 10. Best value: 0.0712397:  36%|███▌      | 18/50 [03:50<10:02, 18.84s/it]
============================================================
Trial 18
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000074
dropout: 0.14428944220076848
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_18 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.034893).  Saving model ...
Updating learning rate to 7.438355669711623e-05
Validation loss decreased (0.034893 --> 0.031143).  Saving model ...
Updating learning rate to 3.7191778348558115e-05
Validation loss decreased (0.031143 --> 0.030169).  Saving model ...
Updating learning rate to 1.8595889174279058e-05
Validation loss decreased (0.030169 --> 0.030037).  Saving model ...
Updating learning rate to 9.297944587139529e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.648972293569764e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.324486146784882e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_18 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05680262669920921, mae:0.0903162732720375, dtw:Not calculated


                                                                                      [I 2025-10-16 22:33:28,379] Trial 18 finished with value: 0.0903162732720375 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 7.438355669711623e-05, 'dropout': 0.14428944220076848}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  36%|███▌      | 18/50 [04:09<10:02, 18.84s/it]Best trial: 10. Best value: 0.0712397:  36%|███▌      | 18/50 [04:09<10:02, 18.84s/it]Best trial: 10. Best value: 0.0712397:  38%|███▊      | 19/50 [04:09<09:42, 18.78s/it]
============================================================
Trial 19
============================================================
seq_len: 48, pred_len: 30
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 64, lr: 0.000565
dropout: 0.22839251523975151
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_19 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037309).  Saving model ...
Updating learning rate to 0.0005648670224939481
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002824335112469741
Validation loss decreased (0.037309 --> 0.037297).  Saving model ...
Updating learning rate to 0.00014121675562348704
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.060837781174352e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.530418890587176e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_19 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.06771434098482132, mae:0.1075374111533165, dtw:Not calculated


                                                                                      [I 2025-10-16 22:33:33,046] Trial 19 finished with value: 0.1075374111533165 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 0.0005648670224939481, 'dropout': 0.22839251523975151}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  38%|███▊      | 19/50 [04:14<09:42, 18.78s/it]Best trial: 10. Best value: 0.0712397:  38%|███▊      | 19/50 [04:14<09:42, 18.78s/it]Best trial: 10. Best value: 0.0712397:  40%|████      | 20/50 [04:14<07:16, 14.54s/it]
============================================================
Trial 20
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000309
dropout: 0.06506258689249297
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_20 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.027290).  Saving model ...
Updating learning rate to 0.00030872505192730586
Validation loss decreased (0.027290 --> 0.027109).  Saving model ...
Updating learning rate to 0.00015436252596365293
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.718126298182647e-05
Validation loss decreased (0.027109 --> 0.026877).  Saving model ...
Updating learning rate to 3.859063149091323e-05
Validation loss decreased (0.026877 --> 0.026380).  Saving model ...
Updating learning rate to 1.9295315745456616e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.647657872728308e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.823828936364154e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_20 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050576839596033096, mae:0.0740182176232338, dtw:Not calculated


                                                                                      [I 2025-10-16 22:33:54,499] Trial 20 finished with value: 0.0740182176232338 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00030872505192730586, 'dropout': 0.06506258689249297}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  40%|████      | 20/50 [04:35<07:16, 14.54s/it]Best trial: 10. Best value: 0.0712397:  40%|████      | 20/50 [04:35<07:16, 14.54s/it]Best trial: 10. Best value: 0.0712397:  42%|████▏     | 21/50 [04:35<08:01, 16.62s/it]
============================================================
Trial 21
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000690
dropout: 0.15565358208230592
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_21 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026874).  Saving model ...
Updating learning rate to 0.0006899051400781505
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034495257003907523
Validation loss decreased (0.026874 --> 0.024905).  Saving model ...
Updating learning rate to 0.00017247628501953761
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.623814250976881e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.3119071254884404e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_21 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04995281994342804, mae:0.07402738928794861, dtw:Not calculated


                                                                                      [I 2025-10-16 22:34:10,665] Trial 21 finished with value: 0.07402738928794861 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0006899051400781505, 'dropout': 0.15565358208230592}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  42%|████▏     | 21/50 [04:51<08:01, 16.62s/it]Best trial: 10. Best value: 0.0712397:  42%|████▏     | 21/50 [04:51<08:01, 16.62s/it]Best trial: 10. Best value: 0.0712397:  44%|████▍     | 22/50 [04:51<07:41, 16.48s/it]
============================================================
Trial 22
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000449
dropout: 0.1309443377604352
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_22 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.027611).  Saving model ...
Updating learning rate to 0.0004494544315695064
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002247272157847532
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001123636078923766
Validation loss decreased (0.027611 --> 0.026025).  Saving model ...
Updating learning rate to 5.61818039461883e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.809090197309415e-05
Validation loss decreased (0.026025 --> 0.025814).  Saving model ...
Updating learning rate to 1.4045450986547074e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.022725493273537e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.5113627466367686e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_22 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05027151107788086, mae:0.07504245638847351, dtw:Not calculated


                                                                                      [I 2025-10-16 22:34:34,464] Trial 22 finished with value: 0.07504245638847351 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004494544315695064, 'dropout': 0.1309443377604352}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  44%|████▍     | 22/50 [05:15<07:41, 16.48s/it]Best trial: 10. Best value: 0.0712397:  44%|████▍     | 22/50 [05:15<07:41, 16.48s/it]Best trial: 10. Best value: 0.0712397:  46%|████▌     | 23/50 [05:15<08:24, 18.68s/it]
============================================================
Trial 23
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000765
dropout: 0.09719743525611921
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_23 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.034821).  Saving model ...
Updating learning rate to 0.0007652873437606063
Validation loss decreased (0.034821 --> 0.025643).  Saving model ...
Updating learning rate to 0.00038264367188030313
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00019132183594015157
Validation loss decreased (0.025643 --> 0.024985).  Saving model ...
Updating learning rate to 9.566091797007578e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.783045898503789e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3915229492518946e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_23 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050809476524591446, mae:0.07312572002410889, dtw:Not calculated


                                                                                      [I 2025-10-16 22:34:53,204] Trial 23 finished with value: 0.07312572002410889 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0007652873437606063, 'dropout': 0.09719743525611921}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  46%|████▌     | 23/50 [05:34<08:24, 18.68s/it]Best trial: 10. Best value: 0.0712397:  46%|████▌     | 23/50 [05:34<08:24, 18.68s/it]Best trial: 10. Best value: 0.0712397:  48%|████▊     | 24/50 [05:34<08:06, 18.70s/it]
============================================================
Trial 24
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000376
dropout: 0.15122081173668972
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_24 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026508).  Saving model ...
Updating learning rate to 0.00037616620564970374
Validation loss decreased (0.026508 --> 0.025701).  Saving model ...
Updating learning rate to 0.00018808310282485187
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.404155141242594e-05
Validation loss decreased (0.025701 --> 0.024615).  Saving model ...
Updating learning rate to 4.702077570621297e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.3510387853106484e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.1755193926553242e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_24 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049598511308431625, mae:0.07389792799949646, dtw:Not calculated


                                                                                      [I 2025-10-16 22:35:12,227] Trial 24 finished with value: 0.07389792799949646 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00037616620564970374, 'dropout': 0.15122081173668972}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  48%|████▊     | 24/50 [05:53<08:06, 18.70s/it]Best trial: 10. Best value: 0.0712397:  48%|████▊     | 24/50 [05:53<08:06, 18.70s/it]Best trial: 10. Best value: 0.0712397:  50%|█████     | 25/50 [05:53<07:49, 18.79s/it]
============================================================
Trial 25
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000994
dropout: 0.17271015270538598
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_25 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.027825).  Saving model ...
Updating learning rate to 0.000993817845449538
Validation loss decreased (0.027825 --> 0.026116).  Saving model ...
Updating learning rate to 0.000496908922724769
Validation loss decreased (0.026116 --> 0.025515).  Saving model ...
Updating learning rate to 0.0002484544613623845
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012422723068119225
Validation loss decreased (0.025515 --> 0.025068).  Saving model ...
Updating learning rate to 6.211361534059612e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.105680767029806e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.552840383514903e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_25 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05054812878370285, mae:0.07248126715421677, dtw:Not calculated


                                                                                      [I 2025-10-16 22:35:33,531] Trial 25 finished with value: 0.07248126715421677 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.000993817845449538, 'dropout': 0.17271015270538598}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  50%|█████     | 25/50 [06:14<07:49, 18.79s/it]Best trial: 10. Best value: 0.0712397:  50%|█████     | 25/50 [06:14<07:49, 18.79s/it]Best trial: 10. Best value: 0.0712397:  52%|█████▏    | 26/50 [06:14<07:49, 19.55s/it]
============================================================
Trial 26
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000204
dropout: 0.21952419286616207
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_26 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.029489).  Saving model ...
Updating learning rate to 0.00020373354286186318
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010186677143093159
Validation loss decreased (0.029489 --> 0.027235).  Saving model ...
Updating learning rate to 5.0933385715465794e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5466692857732897e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.2733346428866449e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_26 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05139242112636566, mae:0.08006104081869125, dtw:Not calculated


                                                                                      [I 2025-10-16 22:35:49,546] Trial 26 finished with value: 0.08006104081869125 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00020373354286186318, 'dropout': 0.21952419286616207}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  52%|█████▏    | 26/50 [06:30<07:49, 19.55s/it]Best trial: 10. Best value: 0.0712397:  52%|█████▏    | 26/50 [06:30<07:49, 19.55s/it]Best trial: 10. Best value: 0.0712397:  54%|█████▍    | 27/50 [06:30<07:05, 18.49s/it]
============================================================
Trial 27
============================================================
seq_len: 48, pred_len: 30
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000591
dropout: 0.12713249106329375
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_27 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037751).  Saving model ...
Updating learning rate to 0.0005907965930221093
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029539829651105463
Validation loss decreased (0.037751 --> 0.036169).  Saving model ...
Updating learning rate to 0.00014769914825552732
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.384957412776366e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.692478706388183e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_27 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07269933074712753, mae:0.10793628543615341, dtw:Not calculated


                                                                                      [I 2025-10-16 22:36:05,694] Trial 27 finished with value: 0.10793628543615341 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0005907965930221093, 'dropout': 0.12713249106329375}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  54%|█████▍    | 27/50 [06:46<07:05, 18.49s/it]Best trial: 10. Best value: 0.0712397:  54%|█████▍    | 27/50 [06:46<07:05, 18.49s/it]Best trial: 10. Best value: 0.0712397:  56%|█████▌    | 28/50 [06:46<06:31, 17.79s/it]
============================================================
Trial 28
============================================================
seq_len: 48, pred_len: 7
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000360
dropout: 0.0012335087346607987
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_28 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.038388).  Saving model ...
Updating learning rate to 0.00036036927372106044
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018018463686053022
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.009231843026511e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_28 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06730224937200546, mae:0.09923707693815231, dtw:Not calculated


                                                                                      [I 2025-10-16 22:36:11,573] Trial 28 finished with value: 0.09923707693815231 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00036036927372106044, 'dropout': 0.0012335087346607987}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  56%|█████▌    | 28/50 [06:52<06:31, 17.79s/it]Best trial: 10. Best value: 0.0712397:  56%|█████▌    | 28/50 [06:52<06:31, 17.79s/it]Best trial: 10. Best value: 0.0712397:  58%|█████▊    | 29/50 [06:52<04:58, 14.21s/it]
============================================================
Trial 29
============================================================
seq_len: 96, pred_len: 7
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000662
dropout: 0.06645573684640142
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_29 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.041608).  Saving model ...
Updating learning rate to 0.0006623950696226699
Validation loss decreased (0.041608 --> 0.039092).  Saving model ...
Updating learning rate to 0.00033119753481133495
Validation loss decreased (0.039092 --> 0.038315).  Saving model ...
Updating learning rate to 0.00016559876740566748
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.279938370283374e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.139969185141687e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_29 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06936490535736084, mae:0.10426793247461319, dtw:Not calculated


                                                                                      [I 2025-10-16 22:36:16,125] Trial 29 finished with value: 0.10426793247461319 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 0.0006623950696226699, 'dropout': 0.06645573684640142}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  58%|█████▊    | 29/50 [06:57<04:58, 14.21s/it]Best trial: 10. Best value: 0.0712397:  58%|█████▊    | 29/50 [06:57<04:58, 14.21s/it]Best trial: 10. Best value: 0.0712397:  60%|██████    | 30/50 [06:57<03:46, 11.32s/it]
============================================================
Trial 30
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000133
dropout: 0.08145338647280399
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_30 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029336).  Saving model ...
Updating learning rate to 0.0001332320504815105
Validation loss decreased (0.029336 --> 0.027270).  Saving model ...
Updating learning rate to 6.661602524075525e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.330801262037762e-05
Validation loss decreased (0.027270 --> 0.026509).  Saving model ...
Updating learning rate to 1.665400631018881e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.327003155094406e-06
Validation loss decreased (0.026509 --> 0.026234).  Saving model ...
Updating learning rate to 4.163501577547203e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.0817507887736015e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.0408753943868007e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_30 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05121613293886185, mae:0.07859327644109726, dtw:Not calculated


                                                                                      [I 2025-10-16 22:36:40,256] Trial 30 finished with value: 0.07859327644109726 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0001332320504815105, 'dropout': 0.08145338647280399}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  60%|██████    | 30/50 [07:21<03:46, 11.32s/it]Best trial: 10. Best value: 0.0712397:  60%|██████    | 30/50 [07:21<03:46, 11.32s/it]Best trial: 10. Best value: 0.0712397:  62%|██████▏   | 31/50 [07:21<04:48, 15.16s/it]
============================================================
Trial 31
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000961
dropout: 0.17034182782303364
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_31 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.028923).  Saving model ...
Updating learning rate to 0.0009608395628117075
Validation loss decreased (0.028923 --> 0.025926).  Saving model ...
Updating learning rate to 0.00048041978140585376
Validation loss decreased (0.025926 --> 0.025913).  Saving model ...
Updating learning rate to 0.00024020989070292688
Validation loss decreased (0.025913 --> 0.025737).  Saving model ...
Updating learning rate to 0.00012010494535146344
Validation loss decreased (0.025737 --> 0.025320).  Saving model ...
Updating learning rate to 6.005247267573172e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.002623633786586e-05
Validation loss decreased (0.025320 --> 0.025155).  Saving model ...
Updating learning rate to 1.501311816893293e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.506559084466465e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.7532795422332325e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_31 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04897904023528099, mae:0.07250656932592392, dtw:Not calculated


                                                                                      [I 2025-10-16 22:37:06,956] Trial 31 finished with value: 0.07250656932592392 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009608395628117075, 'dropout': 0.17034182782303364}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  62%|██████▏   | 31/50 [07:47<04:48, 15.16s/it]Best trial: 10. Best value: 0.0712397:  62%|██████▏   | 31/50 [07:47<04:48, 15.16s/it]Best trial: 10. Best value: 0.0712397:  64%|██████▍   | 32/50 [07:47<05:35, 18.62s/it]
============================================================
Trial 32
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000734
dropout: 0.1934853040541872
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_32 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029264).  Saving model ...
Updating learning rate to 0.0007339323023419325
Validation loss decreased (0.029264 --> 0.028265).  Saving model ...
Updating learning rate to 0.00036696615117096624
Validation loss decreased (0.028265 --> 0.026231).  Saving model ...
Updating learning rate to 0.00018348307558548312
Validation loss decreased (0.026231 --> 0.026080).  Saving model ...
Updating learning rate to 9.174153779274156e-05
Validation loss decreased (0.026080 --> 0.026036).  Saving model ...
Updating learning rate to 4.587076889637078e-05
Validation loss decreased (0.026036 --> 0.025587).  Saving model ...
Updating learning rate to 2.293538444818539e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.1467692224092695e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.7338461120463476e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_32 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05029740184545517, mae:0.07277015596628189, dtw:Not calculated


                                                                                      [I 2025-10-16 22:37:31,035] Trial 32 finished with value: 0.07277015596628189 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0007339323023419325, 'dropout': 0.1934853040541872}. Best is trial 10 with value: 0.0712396502494812.
Best trial: 10. Best value: 0.0712397:  64%|██████▍   | 32/50 [08:12<05:35, 18.62s/it]Best trial: 10. Best value: 0.0712397:  64%|██████▍   | 32/50 [08:12<05:35, 18.62s/it]Best trial: 10. Best value: 0.0712397:  66%|██████▌   | 33/50 [08:12<05:44, 20.26s/it]
============================================================
Trial 33
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000971
dropout: 0.17092911108026365
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_33 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.027832).  Saving model ...
Updating learning rate to 0.0009713675310765985
Validation loss decreased (0.027832 --> 0.026460).  Saving model ...
Updating learning rate to 0.00048568376553829925
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024284188276914962
Validation loss decreased (0.026460 --> 0.025730).  Saving model ...
Updating learning rate to 0.00012142094138457481
Validation loss decreased (0.025730 --> 0.025666).  Saving model ...
Updating learning rate to 6.0710470692287406e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.0355235346143703e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5177617673071851e-05
Validation loss decreased (0.025666 --> 0.025551).  Saving model ...
Updating learning rate to 7.588808836535926e-06
Validation loss decreased (0.025551 --> 0.025231).  Saving model ...
Updating learning rate to 3.794404418267963e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.8972022091339814e-06
>>>>>> Testing on validation set: trial_33 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0488402284681797, mae:0.07091018557548523, dtw:Not calculated


                                                                                      [I 2025-10-16 22:37:57,749] Trial 33 finished with value: 0.07091018557548523 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009713675310765985, 'dropout': 0.17092911108026365}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 10. Best value: 0.0712397:  66%|██████▌   | 33/50 [08:38<05:44, 20.26s/it]Best trial: 33. Best value: 0.0709102:  66%|██████▌   | 33/50 [08:38<05:44, 20.26s/it]Best trial: 33. Best value: 0.0709102:  68%|██████▊   | 34/50 [08:38<05:55, 22.20s/it]
============================================================
Trial 34
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000436
dropout: 0.10701440683794963
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_34 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.038021).  Saving model ...
Updating learning rate to 0.0004362293207578907
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00021811466037894534
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010905733018947267
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_34 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06468802690505981, mae:0.09968997538089752, dtw:Not calculated


                                                                                      [I 2025-10-16 22:38:08,625] Trial 34 finished with value: 0.09968997538089752 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004362293207578907, 'dropout': 0.10701440683794963}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  68%|██████▊   | 34/50 [08:49<05:55, 22.20s/it]Best trial: 33. Best value: 0.0709102:  68%|██████▊   | 34/50 [08:49<05:55, 22.20s/it]Best trial: 33. Best value: 0.0709102:  70%|███████   | 35/50 [08:49<04:41, 18.80s/it]
============================================================
Trial 35
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000263
dropout: 0.154338917841703
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_35 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.030685).  Saving model ...
Updating learning rate to 0.000262518834031971
Validation loss decreased (0.030685 --> 0.029650).  Saving model ...
Updating learning rate to 0.0001312594170159855
Validation loss decreased (0.029650 --> 0.028782).  Saving model ...
Updating learning rate to 6.562970850799274e-05
Validation loss decreased (0.028782 --> 0.026664).  Saving model ...
Updating learning rate to 3.281485425399637e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.6407427126998186e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.203713563499093e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_35 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05148397386074066, mae:0.07688646763563156, dtw:Not calculated


                                                                                      [I 2025-10-16 22:38:27,447] Trial 35 finished with value: 0.07688646763563156 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.000262518834031971, 'dropout': 0.154338917841703}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  70%|███████   | 35/50 [09:08<04:41, 18.80s/it]Best trial: 33. Best value: 0.0709102:  70%|███████   | 35/50 [09:08<04:41, 18.80s/it]Best trial: 33. Best value: 0.0709102:  72%|███████▏  | 36/50 [09:08<04:23, 18.81s/it]
============================================================
Trial 36
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000614
dropout: 0.13152121990013732
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_36 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.025422).  Saving model ...
Updating learning rate to 0.0006140751721947279
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00030703758609736397
Validation loss decreased (0.025422 --> 0.023774).  Saving model ...
Updating learning rate to 0.00015351879304868198
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.675939652434099e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.8379698262170496e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_36 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04979748651385307, mae:0.07420586794614792, dtw:Not calculated


                                                                                      [I 2025-10-16 22:38:36,140] Trial 36 finished with value: 0.07420586794614792 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0006140751721947279, 'dropout': 0.13152121990013732}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  72%|███████▏  | 36/50 [09:17<04:23, 18.81s/it]Best trial: 33. Best value: 0.0709102:  72%|███████▏  | 36/50 [09:17<04:23, 18.81s/it]Best trial: 33. Best value: 0.0709102:  74%|███████▍  | 37/50 [09:17<03:25, 15.77s/it]
============================================================
Trial 37
============================================================
seq_len: 48, pred_len: 14
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000010
dropout: 0.20854144974805802
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_37 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.037447).  Saving model ...
Updating learning rate to 1.0002080162634728e-05
Validation loss decreased (0.037447 --> 0.037092).  Saving model ...
Updating learning rate to 5.001040081317364e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.500520040658682e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.250260020329341e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_37 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06472785025835037, mae:0.09920208156108856, dtw:Not calculated


                                                                                      [I 2025-10-16 22:38:49,827] Trial 37 finished with value: 0.09920208156108856 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 1.0002080162634728e-05, 'dropout': 0.20854144974805802}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  74%|███████▍  | 37/50 [09:30<03:25, 15.77s/it]Best trial: 33. Best value: 0.0709102:  74%|███████▍  | 37/50 [09:30<03:25, 15.77s/it]Best trial: 33. Best value: 0.0709102:  76%|███████▌  | 38/50 [09:30<03:01, 15.15s/it]
============================================================
Trial 38
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000770
dropout: 0.2412264604032605
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_38 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.037393).  Saving model ...
Updating learning rate to 0.0007701039032598614
Validation loss decreased (0.037393 --> 0.037078).  Saving model ...
Updating learning rate to 0.0003850519516299307
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00019252597581496535
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.626298790748267e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_38 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06439041346311569, mae:0.09688378125429153, dtw:Not calculated


                                                                                      [I 2025-10-16 22:38:57,175] Trial 38 finished with value: 0.09688378125429153 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0007701039032598614, 'dropout': 0.2412264604032605}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  76%|███████▌  | 38/50 [09:38<03:01, 15.15s/it]Best trial: 33. Best value: 0.0709102:  76%|███████▌  | 38/50 [09:38<03:01, 15.15s/it]Best trial: 33. Best value: 0.0709102:  78%|███████▊  | 39/50 [09:38<02:20, 12.81s/it]
============================================================
Trial 39
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 64, lr: 0.000029
dropout: 0.17544774434303867
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_39 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.031485).  Saving model ...
Updating learning rate to 2.9353641612478423e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4676820806239212e-05
Validation loss decreased (0.031485 --> 0.029306).  Saving model ...
Updating learning rate to 7.338410403119606e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.669205201559803e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.8346026007799014e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_39 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.06516274809837341, mae:0.11279294639825821, dtw:Not calculated


                                                                                      [I 2025-10-16 22:39:01,808] Trial 39 finished with value: 0.11279294639825821 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 2.9353641612478423e-05, 'dropout': 0.17544774434303867}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  78%|███████▊  | 39/50 [09:42<02:20, 12.81s/it]Best trial: 33. Best value: 0.0709102:  78%|███████▊  | 39/50 [09:42<02:20, 12.81s/it]Best trial: 33. Best value: 0.0709102:  80%|████████  | 40/50 [09:42<01:43, 10.35s/it]
============================================================
Trial 40
============================================================
seq_len: 48, pred_len: 30
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000090
dropout: 0.2678318108720662
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_40 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037476).  Saving model ...
Updating learning rate to 8.960537462709675e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.4802687313548376e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.2401343656774188e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_40 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.06580235064029694, mae:0.10425844043493271, dtw:Not calculated


                                                                                      [I 2025-10-16 22:39:12,740] Trial 40 finished with value: 0.10425844043493271 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 8.960537462709675e-05, 'dropout': 0.2678318108720662}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  80%|████████  | 40/50 [09:53<01:43, 10.35s/it]Best trial: 33. Best value: 0.0709102:  80%|████████  | 40/50 [09:53<01:43, 10.35s/it]Best trial: 33. Best value: 0.0709102:  82%|████████▏ | 41/50 [09:53<01:34, 10.53s/it]
============================================================
Trial 41
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000945
dropout: 0.17280357778758101
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_41 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029787).  Saving model ...
Updating learning rate to 0.0009450024389213838
Validation loss decreased (0.029787 --> 0.029661).  Saving model ...
Updating learning rate to 0.0004725012194606919
Validation loss decreased (0.029661 --> 0.027275).  Saving model ...
Updating learning rate to 0.00023625060973034596
Validation loss decreased (0.027275 --> 0.025900).  Saving model ...
Updating learning rate to 0.00011812530486517298
Validation loss decreased (0.025900 --> 0.025667).  Saving model ...
Updating learning rate to 5.906265243258649e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.9531326216293245e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.4765663108146623e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_41 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05025076866149902, mae:0.07395852357149124, dtw:Not calculated


                                                                                      [I 2025-10-16 22:39:34,186] Trial 41 finished with value: 0.07395852357149124 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009450024389213838, 'dropout': 0.17280357778758101}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  82%|████████▏ | 41/50 [10:15<01:34, 10.53s/it]Best trial: 33. Best value: 0.0709102:  82%|████████▏ | 41/50 [10:15<01:34, 10.53s/it]Best trial: 33. Best value: 0.0709102:  84%|████████▍ | 42/50 [10:15<01:50, 13.80s/it]
============================================================
Trial 42
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000973
dropout: 0.18226472871267602
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_42 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.028778).  Saving model ...
Updating learning rate to 0.0009731773663065089
Validation loss decreased (0.028778 --> 0.025810).  Saving model ...
Updating learning rate to 0.00048658868315325444
Validation loss decreased (0.025810 --> 0.025499).  Saving model ...
Updating learning rate to 0.00024329434157662722
Validation loss decreased (0.025499 --> 0.025016).  Saving model ...
Updating learning rate to 0.00012164717078831361
Validation loss decreased (0.025016 --> 0.024987).  Saving model ...
Updating learning rate to 6.0823585394156804e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.0411792697078402e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5205896348539201e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_42 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04959835484623909, mae:0.07364107668399811, dtw:Not calculated


                                                                                      [I 2025-10-16 22:39:55,638] Trial 42 finished with value: 0.07364107668399811 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009731773663065089, 'dropout': 0.18226472871267602}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  84%|████████▍ | 42/50 [10:36<01:50, 13.80s/it]Best trial: 33. Best value: 0.0709102:  84%|████████▍ | 42/50 [10:36<01:50, 13.80s/it]Best trial: 33. Best value: 0.0709102:  86%|████████▌ | 43/50 [10:36<01:52, 16.10s/it]
============================================================
Trial 43
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000776
dropout: 0.1417104835602529
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_43 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029829).  Saving model ...
Updating learning rate to 0.0007762800555464242
Validation loss decreased (0.029829 --> 0.026238).  Saving model ...
Updating learning rate to 0.0003881400277732121
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00019407001388660605
Validation loss decreased (0.026238 --> 0.025901).  Saving model ...
Updating learning rate to 9.703500694330302e-05
Validation loss decreased (0.025901 --> 0.025488).  Saving model ...
Updating learning rate to 4.851750347165151e-05
Validation loss decreased (0.025488 --> 0.025376).  Saving model ...
Updating learning rate to 2.4258751735825756e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.2129375867912878e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.064687933956439e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_43 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05004652962088585, mae:0.07262155413627625, dtw:Not calculated


                                                                                      [I 2025-10-16 22:40:19,605] Trial 43 finished with value: 0.07262155413627625 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0007762800555464242, 'dropout': 0.1417104835602529}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  86%|████████▌ | 43/50 [11:00<01:52, 16.10s/it]Best trial: 33. Best value: 0.0709102:  86%|████████▌ | 43/50 [11:00<01:52, 16.10s/it]Best trial: 33. Best value: 0.0709102:  88%|████████▊ | 44/50 [11:00<01:50, 18.46s/it]
============================================================
Trial 44
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000580
dropout: 0.11847393342184798
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_44 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.028021).  Saving model ...
Updating learning rate to 0.0005804213434190357
Validation loss decreased (0.028021 --> 0.026994).  Saving model ...
Updating learning rate to 0.00029021067170951787
Validation loss decreased (0.026994 --> 0.026843).  Saving model ...
Updating learning rate to 0.00014510533585475894
Validation loss decreased (0.026843 --> 0.026351).  Saving model ...
Updating learning rate to 7.255266792737947e-05
Validation loss decreased (0.026351 --> 0.026287).  Saving model ...
Updating learning rate to 3.6276333963689734e-05
Validation loss decreased (0.026287 --> 0.025975).  Saving model ...
Updating learning rate to 1.8138166981844867e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.069083490922433e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.534541745461217e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_44 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050938673317432404, mae:0.07396052777767181, dtw:Not calculated


                                                                                      [I 2025-10-16 22:40:43,649] Trial 44 finished with value: 0.07396052777767181 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0005804213434190357, 'dropout': 0.11847393342184798}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  88%|████████▊ | 44/50 [11:24<01:50, 18.46s/it]Best trial: 33. Best value: 0.0709102:  88%|████████▊ | 44/50 [11:24<01:50, 18.46s/it]Best trial: 33. Best value: 0.0709102:  90%|█████████ | 45/50 [11:24<01:40, 20.13s/it]
============================================================
Trial 45
============================================================
seq_len: 48, pred_len: 14
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000353
dropout: 0.1636511020415124
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_45 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.038893).  Saving model ...
Updating learning rate to 0.0003525206956995046
Validation loss decreased (0.038893 --> 0.038780).  Saving model ...
Updating learning rate to 0.0001762603478497523
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.813017392487615e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.406508696243808e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_45 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.0658121109008789, mae:0.10047382861375809, dtw:Not calculated


                                                                                      [I 2025-10-16 22:40:57,256] Trial 45 finished with value: 0.10047382861375809 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0003525206956995046, 'dropout': 0.1636511020415124}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  90%|█████████ | 45/50 [11:38<01:40, 20.13s/it]Best trial: 33. Best value: 0.0709102:  90%|█████████ | 45/50 [11:38<01:40, 20.13s/it]Best trial: 33. Best value: 0.0709102:  92%|█████████▏| 46/50 [11:38<01:12, 18.18s/it]
============================================================
Trial 46
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000525
dropout: 0.10435781145710857
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_46 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.038555).  Saving model ...
Updating learning rate to 0.0005248994525620105
Validation loss decreased (0.038555 --> 0.025517).  Saving model ...
Updating learning rate to 0.0002624497262810053
Validation loss decreased (0.025517 --> 0.024998).  Saving model ...
Updating learning rate to 0.00013122486314050264
Validation loss decreased (0.024998 --> 0.023975).  Saving model ...
Updating learning rate to 6.561243157025132e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.280621578512566e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.640310789256283e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_46 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04967384785413742, mae:0.07438312470912933, dtw:Not calculated


                                                                                      [I 2025-10-16 22:41:07,021] Trial 46 finished with value: 0.07438312470912933 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0005248994525620105, 'dropout': 0.10435781145710857}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  92%|█████████▏| 46/50 [11:48<01:12, 18.18s/it]Best trial: 33. Best value: 0.0709102:  92%|█████████▏| 46/50 [11:48<01:12, 18.18s/it]Best trial: 33. Best value: 0.0709102:  94%|█████████▍| 47/50 [11:48<00:46, 15.65s/it]
============================================================
Trial 47
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000057
dropout: 0.19499130904498663
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_47 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.035453).  Saving model ...
Updating learning rate to 5.7084045490867364e-05
Validation loss decreased (0.035453 --> 0.032701).  Saving model ...
Updating learning rate to 2.8542022745433682e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4271011372716841e-05
Validation loss decreased (0.032701 --> 0.032034).  Saving model ...
Updating learning rate to 7.1355056863584205e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.5677528431792103e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.7838764215896051e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_47 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.059762097895145416, mae:0.09353701770305634, dtw:Not calculated


                                                                                      [I 2025-10-16 22:41:25,903] Trial 47 finished with value: 0.09353701770305634 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 5.7084045490867364e-05, 'dropout': 0.19499130904498663}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  94%|█████████▍| 47/50 [12:06<00:46, 15.65s/it]Best trial: 33. Best value: 0.0709102:  94%|█████████▍| 47/50 [12:06<00:46, 15.65s/it]Best trial: 33. Best value: 0.0709102:  96%|█████████▌| 48/50 [12:06<00:33, 16.62s/it]
============================================================
Trial 48
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 64, lr: 0.000818
dropout: 0.1383180854059884
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_48 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.031342).  Saving model ...
Updating learning rate to 0.00081831771137039
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000409158855685195
Validation loss decreased (0.031342 --> 0.025286).  Saving model ...
Updating learning rate to 0.0002045794278425975
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010228971392129876
Validation loss decreased (0.025286 --> 0.024909).  Saving model ...
Updating learning rate to 5.114485696064938e-05
Validation loss decreased (0.024909 --> 0.023514).  Saving model ...
Updating learning rate to 2.557242848032469e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.2786214240162345e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.393107120081172e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_48 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0506824254989624, mae:0.075344979763031, dtw:Not calculated


                                                                                      [I 2025-10-16 22:41:32,793] Trial 48 finished with value: 0.075344979763031 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 0.00081831771137039, 'dropout': 0.1383180854059884}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  96%|█████████▌| 48/50 [12:13<00:33, 16.62s/it]Best trial: 33. Best value: 0.0709102:  96%|█████████▌| 48/50 [12:13<00:33, 16.62s/it]Best trial: 33. Best value: 0.0709102:  98%|█████████▊| 49/50 [12:13<00:13, 13.70s/it]
============================================================
Trial 49
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000986
dropout: 0.0496832636700402
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_49 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.037347).  Saving model ...
Updating learning rate to 0.0009864608585737524
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004932304292868762
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002466152146434381
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_49 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.0691400095820427, mae:0.09916749596595764, dtw:Not calculated


                                                                                      [I 2025-10-16 22:41:43,844] Trial 49 finished with value: 0.09916749596595764 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009864608585737524, 'dropout': 0.0496832636700402}. Best is trial 33 with value: 0.07091018557548523.
Best trial: 33. Best value: 0.0709102:  98%|█████████▊| 49/50 [12:24<00:13, 13.70s/it]Best trial: 33. Best value: 0.0709102:  98%|█████████▊| 49/50 [12:24<00:13, 13.70s/it]Best trial: 33. Best value: 0.0709102: 100%|██████████| 50/50 [12:24<00:00, 12.92s/it]Best trial: 33. Best value: 0.0709102: 100%|██████████| 50/50 [12:24<00:00, 14.90s/it]

======================================================================
Optimization Complete!
======================================================================
Number of finished trials: 50

Best trial:
  Value (MSE): 0.070910

Best hyperparameters:
  seq_len: 48
  pred_len: 1
  expand: 2
  d_model: 32
  n_heads: 8
  e_layers: 2
  d_layers: 2
  batch_size: 16
  learning_rate: 0.0009713675310765985
  dropout: 0.17092911108026365
======================================================================

✅ Best parameters saved to: ./optuna_results/Mamba_Exchange_1_best_params.json
✅ Visualizations saved to ./optuna_results/

======================================================================
✅ Hyperparameter tuning complete!
✅ Best parameters saved to: ./optuna_results/Mamba_Exchange_1_best_params.json
======================================================================

Next steps:
  1. Review visualizations in optuna_results/ folder
  2. Train final model:
     python train_best_model.py --model Mamba --train_epochs 100
======================================================================
[I 2025-10-16 22:42:03,511] A new study created in RDB with name: Mamba_Exchange_7

======================================================================
Starting Optuna Hyperparameter Optimization
======================================================================
Model: Mamba
Dataset: custom
Number of trials: 50
Study name: Mamba_Exchange_7
Storage: sqlite:///optuna_study.db
======================================================================

  0%|          | 0/50 [00:00<?, ?it/s]
============================================================
Trial 0
============================================================
seq_len: 48, pred_len: 14
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000198
dropout: 0.09790455285690103
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_0 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.039471).  Saving model ...
Updating learning rate to 0.00019767573714183948
Validation loss decreased (0.039471 --> 0.039188).  Saving model ...
Updating learning rate to 9.883786857091974e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.941893428545987e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.4709467142729935e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_0 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06552986055612564, mae:0.10063133388757706, dtw:Not calculated


                                      [I 2025-10-16 22:42:19,703] Trial 0 finished with value: 0.10063133388757706 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00019767573714183948, 'dropout': 0.09790455285690103}. Best is trial 0 with value: 0.10063133388757706.
  0%|          | 0/50 [00:16<?, ?it/s]Best trial: 0. Best value: 0.100631:   0%|          | 0/50 [00:16<?, ?it/s]Best trial: 0. Best value: 0.100631:   2%|▏         | 1/50 [00:16<13:13, 16.20s/it]
============================================================
Trial 1
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000030
dropout: 0.22782613171080454
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_1 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.036998).  Saving model ...
Updating learning rate to 3.0363606988263756e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5181803494131878e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.590901747065939e-06
Validation loss decreased (0.036998 --> 0.032203).  Saving model ...
Updating learning rate to 3.7954508735329695e-06
Validation loss decreased (0.032203 --> 0.028735).  Saving model ...
Updating learning rate to 1.8977254367664848e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.488627183832424e-07
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.744313591916212e-07
Validation loss decreased (0.028735 --> 0.028428).  Saving model ...
Updating learning rate to 2.372156795958106e-07
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.186078397979053e-07
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.930391989895265e-08
>>>>>> Testing on validation set: trial_1 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.06056907773017883, mae:0.09482505172491074, dtw:Not calculated


                                                                                   [I 2025-10-16 22:42:27,164] Trial 1 finished with value: 0.09482505172491074 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 3.0363606988263756e-05, 'dropout': 0.22782613171080454}. Best is trial 1 with value: 0.09482505172491074.
Best trial: 0. Best value: 0.100631:   2%|▏         | 1/50 [00:23<13:13, 16.20s/it]Best trial: 1. Best value: 0.0948251:   2%|▏         | 1/50 [00:23<13:13, 16.20s/it]Best trial: 1. Best value: 0.0948251:   4%|▍         | 2/50 [00:23<08:50, 11.06s/it]
============================================================
Trial 2
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000110
dropout: 0.2907613144008925
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_2 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.028183).  Saving model ...
Updating learning rate to 0.00011014646817281848
Validation loss decreased (0.028183 --> 0.027953).  Saving model ...
Updating learning rate to 5.507323408640924e-05
Validation loss decreased (0.027953 --> 0.026883).  Saving model ...
Updating learning rate to 2.753661704320462e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.376830852160231e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.884154260801155e-06
Validation loss decreased (0.026883 --> 0.026566).  Saving model ...
Updating learning rate to 3.4420771304005774e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.7210385652002887e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.605192826001444e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_2 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050771597772836685, mae:0.07989214360713959, dtw:Not calculated


                                                                                    [I 2025-10-16 22:42:51,234] Trial 2 finished with value: 0.07989214360713959 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00011014646817281848, 'dropout': 0.2907613144008925}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 1. Best value: 0.0948251:   4%|▍         | 2/50 [00:47<08:50, 11.06s/it]Best trial: 2. Best value: 0.0798921:   4%|▍         | 2/50 [00:47<08:50, 11.06s/it]Best trial: 2. Best value: 0.0798921:   6%|▌         | 3/50 [00:47<13:19, 17.00s/it]
============================================================
Trial 3
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000015
dropout: 0.1900763305805142
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_3 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.033068).  Saving model ...
Updating learning rate to 1.4857136803742656e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.428568401871328e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.714284200935664e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_3 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.06106042489409447, mae:0.0953453928232193, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:02,108] Trial 3 finished with value: 0.0953453928232193 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 1.4857136803742656e-05, 'dropout': 0.1900763305805142}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:   6%|▌         | 3/50 [00:58<13:19, 17.00s/it]Best trial: 2. Best value: 0.0798921:   6%|▌         | 3/50 [00:58<13:19, 17.00s/it]Best trial: 2. Best value: 0.0798921:   8%|▊         | 4/50 [00:58<11:10, 14.58s/it]
============================================================
Trial 4
============================================================
seq_len: 48, pred_len: 30
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000038
dropout: 0.20810657874145258
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_4 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037694).  Saving model ...
Updating learning rate to 3.8099881090742984e-05
Validation loss decreased (0.037694 --> 0.037557).  Saving model ...
Updating learning rate to 1.9049940545371492e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.524970272685746e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.762485136342873e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_4 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.06437903642654419, mae:0.10384838283061981, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:09,262] Trial 4 finished with value: 0.10384838283061981 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 3.8099881090742984e-05, 'dropout': 0.20810657874145258}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:   8%|▊         | 4/50 [01:05<11:10, 14.58s/it]Best trial: 2. Best value: 0.0798921:   8%|▊         | 4/50 [01:05<11:10, 14.58s/it]Best trial: 2. Best value: 0.0798921:  10%|█         | 5/50 [01:05<08:56, 11.92s/it]
============================================================
Trial 5
============================================================
seq_len: 96, pred_len: 7
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000087
dropout: 0.28737967519120544
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_5 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.037843).  Saving model ...
Updating learning rate to 8.665489857031511e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.3327449285157554e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.1663724642578777e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_5 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06672129034996033, mae:0.11490605026483536, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:12,584] Trial 5 finished with value: 0.11490605026483536 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 8.665489857031511e-05, 'dropout': 0.28737967519120544}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  10%|█         | 5/50 [01:09<08:56, 11.92s/it]Best trial: 2. Best value: 0.0798921:  10%|█         | 5/50 [01:09<08:56, 11.92s/it]Best trial: 2. Best value: 0.0798921:  12%|█▏        | 6/50 [01:09<06:35,  8.99s/it]
============================================================
Trial 6
============================================================
seq_len: 96, pred_len: 7
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 64, lr: 0.000184
dropout: 0.1348129765463604
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_6 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.037493).  Saving model ...
Updating learning rate to 0.00018400962101538748
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.200481050769374e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.600240525384687e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_6 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06651419401168823, mae:0.1139049157500267, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:16,086] Trial 6 finished with value: 0.1139049157500267 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 0.00018400962101538748, 'dropout': 0.1348129765463604}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  12%|█▏        | 6/50 [01:12<06:35,  8.99s/it]Best trial: 2. Best value: 0.0798921:  12%|█▏        | 6/50 [01:12<06:35,  8.99s/it]Best trial: 2. Best value: 0.0798921:  14%|█▍        | 7/50 [01:12<05:09,  7.19s/it]
============================================================
Trial 7
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000302
dropout: 0.11645761449693877
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_7 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.038755).  Saving model ...
Updating learning rate to 0.0003016024503103791
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015080122515518955
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.540061257759477e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_7 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06576644629240036, mae:0.09878914803266525, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:22,189] Trial 7 finished with value: 0.09878914803266525 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0003016024503103791, 'dropout': 0.11645761449693877}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  14%|█▍        | 7/50 [01:18<05:09,  7.19s/it]Best trial: 2. Best value: 0.0798921:  14%|█▍        | 7/50 [01:18<05:09,  7.19s/it]Best trial: 2. Best value: 0.0798921:  16%|█▌        | 8/50 [01:18<04:47,  6.85s/it]
============================================================
Trial 8
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000183
dropout: 0.2989217203293899
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_8 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.031561).  Saving model ...
Updating learning rate to 0.00018336568529248856
Validation loss decreased (0.031561 --> 0.028702).  Saving model ...
Updating learning rate to 9.168284264624428e-05
Validation loss decreased (0.028702 --> 0.027850).  Saving model ...
Updating learning rate to 4.584142132312214e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.292071066156107e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.1460355330780535e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_8 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05288335680961609, mae:0.08448519557714462, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:38,307] Trial 8 finished with value: 0.08448519557714462 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00018336568529248856, 'dropout': 0.2989217203293899}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  16%|█▌        | 8/50 [01:34<04:47,  6.85s/it]Best trial: 2. Best value: 0.0798921:  16%|█▌        | 8/50 [01:34<04:47,  6.85s/it]Best trial: 2. Best value: 0.0798921:  18%|█▊        | 9/50 [01:34<06:39,  9.74s/it]
============================================================
Trial 9
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000016
dropout: 0.06309085508063351
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_9 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.036373).  Saving model ...
Updating learning rate to 1.6214179367018618e-05
Validation loss decreased (0.036373 --> 0.031753).  Saving model ...
Updating learning rate to 8.107089683509309e-06
Validation loss decreased (0.031753 --> 0.028726).  Saving model ...
Updating learning rate to 4.0535448417546544e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.0267724208773272e-06
Validation loss decreased (0.028726 --> 0.028538).  Saving model ...
Updating learning rate to 1.0133862104386636e-06
Validation loss decreased (0.028538 --> 0.027427).  Saving model ...
Updating learning rate to 5.066931052193318e-07
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.533465526096659e-07
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.2667327630483295e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_9 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.06072727218270302, mae:0.09453454613685608, dtw:Not calculated


                                                                                    [I 2025-10-16 22:43:45,413] Trial 9 finished with value: 0.09453454613685608 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 1.6214179367018618e-05, 'dropout': 0.06309085508063351}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  18%|█▊        | 9/50 [01:41<06:39,  9.74s/it]Best trial: 2. Best value: 0.0798921:  18%|█▊        | 9/50 [01:41<06:39,  9.74s/it]Best trial: 2. Best value: 0.0798921:  20%|██        | 10/50 [01:41<05:57,  8.93s/it]
============================================================
Trial 10
============================================================
seq_len: 96, pred_len: 30
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000878
dropout: 0.0022972454252629115
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_10 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.038159).  Saving model ...
Updating learning rate to 0.0008777198879616734
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004388599439808367
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00021942997199041834
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_10 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07328303158283234, mae:0.1148347333073616, dtw:Not calculated


                                                                                     [I 2025-10-16 22:43:56,359] Trial 10 finished with value: 0.1148347333073616 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0008777198879616734, 'dropout': 0.0022972454252629115}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  20%|██        | 10/50 [01:52<05:57,  8.93s/it]Best trial: 2. Best value: 0.0798921:  20%|██        | 10/50 [01:52<05:57,  8.93s/it]Best trial: 2. Best value: 0.0798921:  22%|██▏       | 11/50 [01:52<06:12,  9.56s/it]
============================================================
Trial 11
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000088
dropout: 0.29749767710768366
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_11 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.028805).  Saving model ...
Updating learning rate to 8.813411443764406e-05
Validation loss decreased (0.028805 --> 0.026092).  Saving model ...
Updating learning rate to 4.406705721882203e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.2033528609411015e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.1016764304705508e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_11 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.052270520478487015, mae:0.08436330407857895, dtw:Not calculated


                                                                                     [I 2025-10-16 22:44:10,565] Trial 11 finished with value: 0.08436330407857895 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 8.813411443764406e-05, 'dropout': 0.29749767710768366}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  22%|██▏       | 11/50 [02:07<06:12,  9.56s/it]Best trial: 2. Best value: 0.0798921:  22%|██▏       | 11/50 [02:07<06:12,  9.56s/it]Best trial: 2. Best value: 0.0798921:  24%|██▍       | 12/50 [02:07<06:56, 10.96s/it]
============================================================
Trial 12
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000057
dropout: 0.2522233184236921
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_12 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.029772).  Saving model ...
Updating learning rate to 5.671133523184747e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.8355667615923735e-05
Validation loss decreased (0.029772 --> 0.027145).  Saving model ...
Updating learning rate to 1.4177833807961867e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.088916903980934e-06
Validation loss decreased (0.027145 --> 0.027119).  Saving model ...
Updating learning rate to 3.544458451990467e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.7722292259952334e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.861146129976167e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_12 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0525861419737339, mae:0.0846254751086235, dtw:Not calculated


                                                                                     [I 2025-10-16 22:44:32,046] Trial 12 finished with value: 0.0846254751086235 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 5.671133523184747e-05, 'dropout': 0.2522233184236921}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  24%|██▍       | 12/50 [02:28<06:56, 10.96s/it]Best trial: 2. Best value: 0.0798921:  24%|██▍       | 12/50 [02:28<06:56, 10.96s/it]Best trial: 2. Best value: 0.0798921:  26%|██▌       | 13/50 [02:28<08:43, 14.15s/it]
============================================================
Trial 13
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000091
dropout: 0.2593780272708915
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_13 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.029373).  Saving model ...
Updating learning rate to 9.112020744806385e-05
Validation loss decreased (0.029373 --> 0.028751).  Saving model ...
Updating learning rate to 4.5560103724031924e-05
Validation loss decreased (0.028751 --> 0.027941).  Saving model ...
Updating learning rate to 2.2780051862015962e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.1390025931007981e-05
Validation loss decreased (0.027941 --> 0.027565).  Saving model ...
Updating learning rate to 5.6950129655039906e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.8475064827519953e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.4237532413759976e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_13 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05112173780798912, mae:0.0819782093167305, dtw:Not calculated


                                                                                     [I 2025-10-16 22:44:53,533] Trial 13 finished with value: 0.0819782093167305 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 9.112020744806385e-05, 'dropout': 0.2593780272708915}. Best is trial 2 with value: 0.07989214360713959.
Best trial: 2. Best value: 0.0798921:  26%|██▌       | 13/50 [02:50<08:43, 14.15s/it]Best trial: 2. Best value: 0.0798921:  26%|██▌       | 13/50 [02:50<08:43, 14.15s/it]Best trial: 2. Best value: 0.0798921:  28%|██▊       | 14/50 [02:50<09:49, 16.37s/it]
============================================================
Trial 14
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000440
dropout: 0.25340759766319
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_14 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.027224).  Saving model ...
Updating learning rate to 0.0004401994243974656
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002200997121987328
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001100498560993664
Validation loss decreased (0.027224 --> 0.027027).  Saving model ...
Updating learning rate to 5.50249280496832e-05
Validation loss decreased (0.027027 --> 0.025914).  Saving model ...
Updating learning rate to 2.75124640248416e-05
Validation loss decreased (0.025914 --> 0.025818).  Saving model ...
Updating learning rate to 1.37562320124208e-05
Validation loss decreased (0.025818 --> 0.025074).  Saving model ...
Updating learning rate to 6.8781160062104e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.4390580031052e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.7195290015526e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_14 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04972917586565018, mae:0.07370081543922424, dtw:Not calculated


                                                                                     [I 2025-10-16 22:45:20,393] Trial 14 finished with value: 0.07370081543922424 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004401994243974656, 'dropout': 0.25340759766319}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 2. Best value: 0.0798921:  28%|██▊       | 14/50 [03:16<09:49, 16.37s/it]Best trial: 14. Best value: 0.0737008:  28%|██▊       | 14/50 [03:16<09:49, 16.37s/it]Best trial: 14. Best value: 0.0737008:  30%|███       | 15/50 [03:16<11:23, 19.53s/it]
============================================================
Trial 15
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000628
dropout: 0.19346793990625788
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_15 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.024506).  Saving model ...
Updating learning rate to 0.0006281189081984079
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031405945409920393
Validation loss decreased (0.024506 --> 0.023872).  Saving model ...
Updating learning rate to 0.00015702972704960196
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.851486352480098e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.925743176240049e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_15 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049678150564432144, mae:0.07412849366664886, dtw:Not calculated


                                                                                      [I 2025-10-16 22:45:28,983] Trial 15 finished with value: 0.07412849366664886 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0006281189081984079, 'dropout': 0.19346793990625788}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  30%|███       | 15/50 [03:25<11:23, 19.53s/it]Best trial: 14. Best value: 0.0737008:  30%|███       | 15/50 [03:25<11:23, 19.53s/it]Best trial: 14. Best value: 0.0737008:  32%|███▏      | 16/50 [03:25<09:11, 16.23s/it]
============================================================
Trial 16
============================================================
seq_len: 96, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000960
dropout: 0.17742224863095035
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_16 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.036802).  Saving model ...
Updating learning rate to 0.000960245983841302
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000480122991920651
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002400614959603255
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_16 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.0667024478316307, mae:0.09952190518379211, dtw:Not calculated


                                                                                      [I 2025-10-16 22:45:34,723] Trial 16 finished with value: 0.09952190518379211 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.000960245983841302, 'dropout': 0.17742224863095035}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  32%|███▏      | 16/50 [03:31<09:11, 16.23s/it]Best trial: 14. Best value: 0.0737008:  32%|███▏      | 16/50 [03:31<09:11, 16.23s/it]Best trial: 14. Best value: 0.0737008:  34%|███▍      | 17/50 [03:31<07:11, 13.08s/it]
============================================================
Trial 17
============================================================
seq_len: 96, pred_len: 30
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000523
dropout: 0.16206973109634354
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_17 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.035277).  Saving model ...
Updating learning rate to 0.0005233276215785263
Validation loss decreased (0.035277 --> 0.034475).  Saving model ...
Updating learning rate to 0.0002616638107892631
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013083190539463156
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.541595269731578e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_17 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07151149958372116, mae:0.11307257413864136, dtw:Not calculated


                                                                                      [I 2025-10-16 22:45:41,684] Trial 17 finished with value: 0.11307257413864136 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0005233276215785263, 'dropout': 0.16206973109634354}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  34%|███▍      | 17/50 [03:38<07:11, 13.08s/it]Best trial: 14. Best value: 0.0737008:  34%|███▍      | 17/50 [03:38<07:11, 13.08s/it]Best trial: 14. Best value: 0.0737008:  36%|███▌      | 18/50 [03:38<05:59, 11.24s/it]
============================================================
Trial 18
============================================================
seq_len: 96, pred_len: 14
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000442
dropout: 0.22472850372635533
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_18 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.040100).  Saving model ...
Updating learning rate to 0.000441746553062826
Validation loss decreased (0.040100 --> 0.039018).  Saving model ...
Updating learning rate to 0.000220873276531413
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001104366382657065
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.521831913285325e-05
Validation loss decreased (0.039018 --> 0.038992).  Saving model ...
Updating learning rate to 2.7609159566426623e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.3804579783213312e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.902289891606656e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_18 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.07214006036520004, mae:0.1050325557589531, dtw:Not calculated


                                                                                      [I 2025-10-16 22:45:53,127] Trial 18 finished with value: 0.1050325557589531 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.000441746553062826, 'dropout': 0.22472850372635533}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  36%|███▌      | 18/50 [03:49<05:59, 11.24s/it]Best trial: 14. Best value: 0.0737008:  36%|███▌      | 18/50 [03:49<05:59, 11.24s/it]Best trial: 14. Best value: 0.0737008:  38%|███▊      | 19/50 [03:49<05:50, 11.30s/it]
============================================================
Trial 19
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000503
dropout: 0.23797782449291696
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_19 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026316).  Saving model ...
Updating learning rate to 0.0005033106338462795
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00025165531692313974
Validation loss decreased (0.026316 --> 0.023591).  Saving model ...
Updating learning rate to 0.00012582765846156987
Validation loss decreased (0.023591 --> 0.023260).  Saving model ...
Updating learning rate to 6.291382923078493e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.145691461539247e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5728457307696234e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_19 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04985743388533592, mae:0.07406791299581528, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:03,328] Trial 19 finished with value: 0.07406791299581528 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0005033106338462795, 'dropout': 0.23797782449291696}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  38%|███▊      | 19/50 [03:59<05:50, 11.30s/it]Best trial: 14. Best value: 0.0737008:  38%|███▊      | 19/50 [03:59<05:50, 11.30s/it]Best trial: 14. Best value: 0.0737008:  40%|████      | 20/50 [03:59<05:29, 10.97s/it]
============================================================
Trial 20
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000344
dropout: 0.2524842228274623
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_20 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.025411).  Saving model ...
Updating learning rate to 0.00034350780997177146
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017175390498588573
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.587695249294287e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_20 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05347924306988716, mae:0.08630599081516266, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:09,018] Trial 20 finished with value: 0.08630599081516266 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00034350780997177146, 'dropout': 0.2524842228274623}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  40%|████      | 20/50 [04:05<05:29, 10.97s/it]Best trial: 14. Best value: 0.0737008:  40%|████      | 20/50 [04:05<05:29, 10.97s/it]Best trial: 14. Best value: 0.0737008:  42%|████▏     | 21/50 [04:05<04:32,  9.39s/it]
============================================================
Trial 21
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000613
dropout: 0.2012440478931155
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_21 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.028136).  Saving model ...
Updating learning rate to 0.000612777488391982
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000306388744195991
Validation loss decreased (0.028136 --> 0.024628).  Saving model ...
Updating learning rate to 0.0001531943720979955
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.659718604899775e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.829859302449888e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_21 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04900451377034187, mae:0.07530764490365982, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:17,660] Trial 21 finished with value: 0.07530764490365982 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.000612777488391982, 'dropout': 0.2012440478931155}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  42%|████▏     | 21/50 [04:14<04:32,  9.39s/it]Best trial: 14. Best value: 0.0737008:  42%|████▏     | 21/50 [04:14<04:32,  9.39s/it]Best trial: 14. Best value: 0.0737008:  44%|████▍     | 22/50 [04:14<04:16,  9.16s/it]
============================================================
Trial 22
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000700
dropout: 0.23206882012886096
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_22 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.025452).  Saving model ...
Updating learning rate to 0.0007001847242514425
Validation loss decreased (0.025452 --> 0.024609).  Saving model ...
Updating learning rate to 0.00035009236212572126
Validation loss decreased (0.024609 --> 0.023654).  Saving model ...
Updating learning rate to 0.00017504618106286063
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.752309053143032e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.376154526571516e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_22 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05004943534731865, mae:0.07511772960424423, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:26,396] Trial 22 finished with value: 0.07511772960424423 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0007001847242514425, 'dropout': 0.23206882012886096}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  44%|████▍     | 22/50 [04:22<04:16,  9.16s/it]Best trial: 14. Best value: 0.0737008:  44%|████▍     | 22/50 [04:22<04:16,  9.16s/it]Best trial: 14. Best value: 0.0737008:  46%|████▌     | 23/50 [04:22<04:03,  9.03s/it]
============================================================
Trial 23
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000300
dropout: 0.15934927505817154
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_23 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.025899).  Saving model ...
Updating learning rate to 0.0002999153581407547
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00014995767907037734
Validation loss decreased (0.025899 --> 0.024530).  Saving model ...
Updating learning rate to 7.497883953518867e-05
Validation loss decreased (0.024530 --> 0.024125).  Saving model ...
Updating learning rate to 3.7489419767594334e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.8744709883797167e-05
Validation loss decreased (0.024125 --> 0.023977).  Saving model ...
Updating learning rate to 9.372354941898584e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.686177470949292e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.343088735474646e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_23 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04947815090417862, mae:0.07447171211242676, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:39,088] Trial 23 finished with value: 0.07447171211242676 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0002999153581407547, 'dropout': 0.15934927505817154}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  46%|████▌     | 23/50 [04:35<04:03,  9.03s/it]Best trial: 14. Best value: 0.0737008:  46%|████▌     | 23/50 [04:35<04:03,  9.03s/it]Best trial: 14. Best value: 0.0737008:  48%|████▊     | 24/50 [04:35<04:23, 10.13s/it]
============================================================
Trial 24
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000453
dropout: 0.2540333087882349
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_24 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026201).  Saving model ...
Updating learning rate to 0.00045285298500519666
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022642649250259833
Validation loss decreased (0.026201 --> 0.024575).  Saving model ...
Updating learning rate to 0.00011321324625129917
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.660662312564958e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.830331156282479e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_24 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.051149144768714905, mae:0.0770244225859642, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:47,681] Trial 24 finished with value: 0.0770244225859642 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00045285298500519666, 'dropout': 0.2540333087882349}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  48%|████▊     | 24/50 [04:44<04:23, 10.13s/it]Best trial: 14. Best value: 0.0737008:  48%|████▊     | 24/50 [04:44<04:23, 10.13s/it]Best trial: 14. Best value: 0.0737008:  50%|█████     | 25/50 [04:44<04:01,  9.67s/it]
============================================================
Trial 25
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000279
dropout: 0.20888599224419876
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_25 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.041515).  Saving model ...
Updating learning rate to 0.00027877104631403294
Validation loss decreased (0.041515 --> 0.025610).  Saving model ...
Updating learning rate to 0.00013938552315701647
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.969276157850823e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.484638078925412e-05
Validation loss decreased (0.025610 --> 0.024182).  Saving model ...
Updating learning rate to 1.742319039462706e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.71159519731353e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.355797598656765e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_25 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05052533000707626, mae:0.07753133028745651, dtw:Not calculated


                                                                                      [I 2025-10-16 22:46:58,929] Trial 25 finished with value: 0.07753133028745651 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00027877104631403294, 'dropout': 0.20888599224419876}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  50%|█████     | 25/50 [04:55<04:01,  9.67s/it]Best trial: 14. Best value: 0.0737008:  50%|█████     | 25/50 [04:55<04:01,  9.67s/it]Best trial: 14. Best value: 0.0737008:  52%|█████▏    | 26/50 [04:55<04:03, 10.14s/it]
============================================================
Trial 26
============================================================
seq_len: 96, pred_len: 14
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000731
dropout: 0.2747748675256483
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_26 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.038343).  Saving model ...
Updating learning rate to 0.0007313119288447006
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003656559644223503
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018282798221117515
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_26 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06723984330892563, mae:0.10609954595565796, dtw:Not calculated


                                                                                      [I 2025-10-16 22:47:04,815] Trial 26 finished with value: 0.10609954595565796 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0007313119288447006, 'dropout': 0.2747748675256483}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  52%|█████▏    | 26/50 [05:01<04:03, 10.14s/it]Best trial: 14. Best value: 0.0737008:  52%|█████▏    | 26/50 [05:01<04:03, 10.14s/it]Best trial: 14. Best value: 0.0737008:  54%|█████▍    | 27/50 [05:01<03:23,  8.87s/it]
============================================================
Trial 27
============================================================
seq_len: 96, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000406
dropout: 0.18589408808335747
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_27 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.037859).  Saving model ...
Updating learning rate to 0.00040615917721224175
Validation loss decreased (0.037859 --> 0.037236).  Saving model ...
Updating learning rate to 0.00020307958860612087
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010153979430306044
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.076989715153022e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_27 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.0667327418923378, mae:0.0988629087805748, dtw:Not calculated


                                                                                      [I 2025-10-16 22:47:12,193] Trial 27 finished with value: 0.0988629087805748 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00040615917721224175, 'dropout': 0.18589408808335747}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  54%|█████▍    | 27/50 [05:08<03:23,  8.87s/it]Best trial: 14. Best value: 0.0737008:  54%|█████▍    | 27/50 [05:08<03:23,  8.87s/it]Best trial: 14. Best value: 0.0737008:  56%|█████▌    | 28/50 [05:08<03:05,  8.42s/it]
============================================================
Trial 28
============================================================
seq_len: 96, pred_len: 30
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000228
dropout: 0.22080968613133123
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_28 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.036454).  Saving model ...
Updating learning rate to 0.00022781914106655813
Validation loss decreased (0.036454 --> 0.035396).  Saving model ...
Updating learning rate to 0.00011390957053327906
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.695478526663953e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.8477392633319766e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_28 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07101879268884659, mae:0.11193715780973434, dtw:Not calculated


                                                                                      [I 2025-10-16 22:47:16,259] Trial 28 finished with value: 0.11193715780973434 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 0.00022781914106655813, 'dropout': 0.22080968613133123}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  56%|█████▌    | 28/50 [05:12<03:05,  8.42s/it]Best trial: 14. Best value: 0.0737008:  56%|█████▌    | 28/50 [05:12<03:05,  8.42s/it]Best trial: 14. Best value: 0.0737008:  58%|█████▊    | 29/50 [05:12<02:29,  7.11s/it]
============================================================
Trial 29
============================================================
seq_len: 48, pred_len: 14
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000140
dropout: 0.061105657769497834
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_29 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.038395).  Saving model ...
Updating learning rate to 0.00013975999699960133
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.987999849980066e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.493999924990033e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_29 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06452494859695435, mae:0.09872695803642273, dtw:Not calculated


                                                                                      [I 2025-10-16 22:47:27,271] Trial 29 finished with value: 0.09872695803642273 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00013975999699960133, 'dropout': 0.061105657769497834}. Best is trial 14 with value: 0.07370081543922424.
Best trial: 14. Best value: 0.0737008:  58%|█████▊    | 29/50 [05:23<02:29,  7.11s/it]Best trial: 14. Best value: 0.0737008:  58%|█████▊    | 29/50 [05:23<02:29,  7.11s/it]Best trial: 14. Best value: 0.0737008:  60%|██████    | 30/50 [05:23<02:45,  8.28s/it]
============================================================
Trial 30
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000998
dropout: 0.2696695712685458
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_30 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026292).  Saving model ...
Updating learning rate to 0.0009983794722954467
Validation loss decreased (0.026292 --> 0.025963).  Saving model ...
Updating learning rate to 0.0004991897361477233
Validation loss decreased (0.025963 --> 0.024738).  Saving model ...
Updating learning rate to 0.00024959486807386166
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012479743403693083
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.239871701846542e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_30 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04947061464190483, mae:0.07333198934793472, dtw:Not calculated


                                                                                      [I 2025-10-16 22:47:43,862] Trial 30 finished with value: 0.07333198934793472 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009983794722954467, 'dropout': 0.2696695712685458}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 14. Best value: 0.0737008:  60%|██████    | 30/50 [05:40<02:45,  8.28s/it]Best trial: 30. Best value: 0.073332:  60%|██████    | 30/50 [05:40<02:45,  8.28s/it] Best trial: 30. Best value: 0.073332:  62%|██████▏   | 31/50 [05:40<03:24, 10.78s/it]
============================================================
Trial 31
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000990
dropout: 0.2673251311588922
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_31 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026420).  Saving model ...
Updating learning rate to 0.0009895201184226528
Validation loss decreased (0.026420 --> 0.026290).  Saving model ...
Updating learning rate to 0.0004947600592113264
Validation loss decreased (0.026290 --> 0.025544).  Saving model ...
Updating learning rate to 0.0002473800296056632
Validation loss decreased (0.025544 --> 0.024841).  Saving model ...
Updating learning rate to 0.0001236900148028316
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.18450074014158e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.09225037007079e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_31 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0501389317214489, mae:0.07418499141931534, dtw:Not calculated


                                                                                     [I 2025-10-16 22:48:03,225] Trial 31 finished with value: 0.07418499141931534 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009895201184226528, 'dropout': 0.2673251311588922}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  62%|██████▏   | 31/50 [05:59<03:24, 10.78s/it]Best trial: 30. Best value: 0.073332:  62%|██████▏   | 31/50 [05:59<03:24, 10.78s/it]Best trial: 30. Best value: 0.073332:  64%|██████▍   | 32/50 [05:59<04:00, 13.36s/it]
============================================================
Trial 32
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000603
dropout: 0.23621157940192875
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_32 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.027583).  Saving model ...
Updating learning rate to 0.0006027150495907645
Validation loss decreased (0.027583 --> 0.026306).  Saving model ...
Updating learning rate to 0.00030135752479538226
Validation loss decreased (0.026306 --> 0.025809).  Saving model ...
Updating learning rate to 0.00015067876239769113
Validation loss decreased (0.025809 --> 0.025308).  Saving model ...
Updating learning rate to 7.533938119884557e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.766969059942278e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.883484529971139e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_32 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05079898238182068, mae:0.0738949105143547, dtw:Not calculated


                                                                                     [I 2025-10-16 22:48:22,539] Trial 32 finished with value: 0.0738949105143547 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0006027150495907645, 'dropout': 0.23621157940192875}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  64%|██████▍   | 32/50 [06:19<04:00, 13.36s/it]Best trial: 30. Best value: 0.073332:  64%|██████▍   | 32/50 [06:19<04:00, 13.36s/it]Best trial: 30. Best value: 0.073332:  66%|██████▌   | 33/50 [06:19<04:17, 15.14s/it]
============================================================
Trial 33
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000549
dropout: 0.23882155235391714
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_33 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026316).  Saving model ...
Updating learning rate to 0.0005492764693843701
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002746382346921851
Validation loss decreased (0.026316 --> 0.026040).  Saving model ...
Updating learning rate to 0.00013731911734609254
Validation loss decreased (0.026040 --> 0.025648).  Saving model ...
Updating learning rate to 6.865955867304627e-05
Validation loss decreased (0.025648 --> 0.025598).  Saving model ...
Updating learning rate to 3.4329779336523134e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.7164889668261567e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.582444834130784e-06
Validation loss decreased (0.025598 --> 0.025324).  Saving model ...
Updating learning rate to 4.291222417065392e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.145611208532696e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.072805604266348e-06
>>>>>> Testing on validation set: trial_33 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050527315586805344, mae:0.0752883031964302, dtw:Not calculated


                                                                                     [I 2025-10-16 22:48:49,977] Trial 33 finished with value: 0.0752883031964302 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0005492764693843701, 'dropout': 0.23882155235391714}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  66%|██████▌   | 33/50 [06:46<04:17, 15.14s/it]Best trial: 30. Best value: 0.073332:  66%|██████▌   | 33/50 [06:46<04:17, 15.14s/it]Best trial: 30. Best value: 0.073332:  68%|██████▊   | 34/50 [06:46<05:01, 18.83s/it]
============================================================
Trial 34
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000761
dropout: 0.242718138206682
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_34 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.030948).  Saving model ...
Updating learning rate to 0.0007612852039587333
Validation loss decreased (0.030948 --> 0.026865).  Saving model ...
Updating learning rate to 0.00038064260197936663
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00019032130098968331
Validation loss decreased (0.026865 --> 0.025155).  Saving model ...
Updating learning rate to 9.516065049484166e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.758032524742083e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3790162623710414e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_34 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05004057288169861, mae:0.07405493408441544, dtw:Not calculated


                                                                                     [I 2025-10-16 22:49:09,083] Trial 34 finished with value: 0.07405493408441544 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0007612852039587333, 'dropout': 0.242718138206682}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  68%|██████▊   | 34/50 [07:05<05:01, 18.83s/it]Best trial: 30. Best value: 0.073332:  68%|██████▊   | 34/50 [07:05<05:01, 18.83s/it]Best trial: 30. Best value: 0.073332:  70%|███████   | 35/50 [07:05<04:43, 18.91s/it]
============================================================
Trial 35
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000767
dropout: 0.2775959792709205
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_35 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.028528).  Saving model ...
Updating learning rate to 0.0007673543774290591
Validation loss decreased (0.028528 --> 0.026885).  Saving model ...
Updating learning rate to 0.00038367718871452954
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00019183859435726477
Validation loss decreased (0.026885 --> 0.026593).  Saving model ...
Updating learning rate to 9.591929717863238e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.795964858931619e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3979824294658096e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_35 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05099620670080185, mae:0.07613475620746613, dtw:Not calculated


                                                                                     [I 2025-10-16 22:49:28,009] Trial 35 finished with value: 0.07613475620746613 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0007673543774290591, 'dropout': 0.2775959792709205}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  70%|███████   | 35/50 [07:24<04:43, 18.91s/it]Best trial: 30. Best value: 0.073332:  70%|███████   | 35/50 [07:24<04:43, 18.91s/it]Best trial: 30. Best value: 0.073332:  72%|███████▏  | 36/50 [07:24<04:24, 18.92s/it]
============================================================
Trial 36
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000759
dropout: 0.24521654244529137
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_36 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.027429).  Saving model ...
Updating learning rate to 0.0007592175628482102
Validation loss decreased (0.027429 --> 0.025706).  Saving model ...
Updating learning rate to 0.0003796087814241051
Validation loss decreased (0.025706 --> 0.025041).  Saving model ...
Updating learning rate to 0.00018980439071205254
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.490219535602627e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7451097678013136e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_36 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04961813986301422, mae:0.0756065770983696, dtw:Not calculated


                                                                                     [I 2025-10-16 22:49:44,651] Trial 36 finished with value: 0.0756065770983696 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0007592175628482102, 'dropout': 0.24521654244529137}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  72%|███████▏  | 36/50 [07:41<04:24, 18.92s/it]Best trial: 30. Best value: 0.073332:  72%|███████▏  | 36/50 [07:41<04:24, 18.92s/it]Best trial: 30. Best value: 0.073332:  74%|███████▍  | 37/50 [07:41<03:57, 18.24s/it]
============================================================
Trial 37
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000352
dropout: 0.27508572640342577
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_37 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029163).  Saving model ...
Updating learning rate to 0.0003522878861963901
Validation loss decreased (0.029163 --> 0.027748).  Saving model ...
Updating learning rate to 0.00017614394309819505
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.807197154909752e-05
Validation loss decreased (0.027748 --> 0.027627).  Saving model ...
Updating learning rate to 4.403598577454876e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.201799288727438e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.100899644363719e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_37 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05138767138123512, mae:0.07700308412313461, dtw:Not calculated


                                                                                     [I 2025-10-16 22:50:03,770] Trial 37 finished with value: 0.07700308412313461 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0003522878861963901, 'dropout': 0.27508572640342577}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  74%|███████▍  | 37/50 [08:00<03:57, 18.24s/it]Best trial: 30. Best value: 0.073332:  74%|███████▍  | 37/50 [08:00<03:57, 18.24s/it]Best trial: 30. Best value: 0.073332:  76%|███████▌  | 38/50 [08:00<03:42, 18.50s/it]
============================================================
Trial 38
============================================================
seq_len: 48, pred_len: 7
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000228
dropout: 0.21306602529463264
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_38 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.036798).  Saving model ...
Updating learning rate to 0.0002281692531635333
Validation loss decreased (0.036798 --> 0.036441).  Saving model ...
Updating learning rate to 0.00011408462658176666
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.704231329088333e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.8521156645441664e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_38 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06423334032297134, mae:0.09603727608919144, dtw:Not calculated


                                                                                     [I 2025-10-16 22:50:17,644] Trial 38 finished with value: 0.09603727608919144 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 1, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0002281692531635333, 'dropout': 0.21306602529463264}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  76%|███████▌  | 38/50 [08:14<03:42, 18.50s/it]Best trial: 30. Best value: 0.073332:  76%|███████▌  | 38/50 [08:14<03:42, 18.50s/it]Best trial: 30. Best value: 0.073332:  78%|███████▊  | 39/50 [08:14<03:08, 17.11s/it]
============================================================
Trial 39
============================================================
seq_len: 48, pred_len: 30
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000063
dropout: 0.28036350165123725
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_39 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037877).  Saving model ...
Updating learning rate to 6.349595991322294e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.174797995661147e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5873989978305735e-05
Validation loss decreased (0.037877 --> 0.037777).  Saving model ...
Updating learning rate to 7.936994989152867e-06
Validation loss decreased (0.037777 --> 0.037740).  Saving model ...
Updating learning rate to 3.968497494576434e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.984248747288217e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.921243736441084e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_39 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.0658881664276123, mae:0.10472572594881058, dtw:Not calculated


                                                                                     [I 2025-10-16 22:50:39,360] Trial 39 finished with value: 0.10472572594881058 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 6.349595991322294e-05, 'dropout': 0.28036350165123725}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  78%|███████▊  | 39/50 [08:35<03:08, 17.11s/it]Best trial: 30. Best value: 0.073332:  78%|███████▊  | 39/50 [08:35<03:08, 17.11s/it]Best trial: 30. Best value: 0.073332:  80%|████████  | 40/50 [08:35<03:04, 18.48s/it]
============================================================
Trial 40
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000134
dropout: 0.14440791728146457
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_40 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029005).  Saving model ...
Updating learning rate to 0.00013383997818236924
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.691998909118462e-05
Validation loss decreased (0.029005 --> 0.027887).  Saving model ...
Updating learning rate to 3.345999454559231e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.6729997272796155e-05
Validation loss decreased (0.027887 --> 0.027009).  Saving model ...
Updating learning rate to 8.364998636398078e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.182499318199039e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.0912496590995194e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_40 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05398116260766983, mae:0.08043883740901947, dtw:Not calculated


                                                                                     [I 2025-10-16 22:51:01,010] Trial 40 finished with value: 0.08043883740901947 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.00013383997818236924, 'dropout': 0.14440791728146457}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  80%|████████  | 40/50 [08:57<03:04, 18.48s/it]Best trial: 30. Best value: 0.073332:  80%|████████  | 40/50 [08:57<03:04, 18.48s/it]Best trial: 30. Best value: 0.073332:  82%|████████▏ | 41/50 [08:57<02:54, 19.43s/it]
============================================================
Trial 41
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000508
dropout: 0.23613894287963758
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_41 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.031402).  Saving model ...
Updating learning rate to 0.0005076865873239072
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002538432936619536
Validation loss decreased (0.031402 --> 0.025330).  Saving model ...
Updating learning rate to 0.0001269216468309768
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.34608234154884e-05
Validation loss decreased (0.025330 --> 0.024911).  Saving model ...
Updating learning rate to 3.17304117077442e-05
Validation loss decreased (0.024911 --> 0.024844).  Saving model ...
Updating learning rate to 1.58652058538721e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.93260292693605e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.966301463468025e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_41 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050115860998630524, mae:0.0749044120311737, dtw:Not calculated


                                                                                     [I 2025-10-16 22:51:25,298] Trial 41 finished with value: 0.0749044120311737 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0005076865873239072, 'dropout': 0.23613894287963758}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  82%|████████▏ | 41/50 [09:21<02:54, 19.43s/it]Best trial: 30. Best value: 0.073332:  82%|████████▏ | 41/50 [09:21<02:54, 19.43s/it]Best trial: 30. Best value: 0.073332:  84%|████████▍ | 42/50 [09:21<02:47, 20.89s/it]
============================================================
Trial 42
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000978
dropout: 0.26505663867313467
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_42 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026441).  Saving model ...
Updating learning rate to 0.0009779529584250347
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004889764792125173
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024448823960625867
Validation loss decreased (0.026441 --> 0.024943).  Saving model ...
Updating learning rate to 0.00012224411980312933
Validation loss decreased (0.024943 --> 0.024764).  Saving model ...
Updating learning rate to 6.112205990156467e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.0561029950782334e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5280514975391167e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_42 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.048891015350818634, mae:0.07341280579566956, dtw:Not calculated


                                                                                     [I 2025-10-16 22:51:46,734] Trial 42 finished with value: 0.07341280579566956 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009779529584250347, 'dropout': 0.26505663867313467}. Best is trial 30 with value: 0.07333198934793472.
Best trial: 30. Best value: 0.073332:  84%|████████▍ | 42/50 [09:43<02:47, 20.89s/it]Best trial: 30. Best value: 0.073332:  84%|████████▍ | 42/50 [09:43<02:47, 20.89s/it]Best trial: 30. Best value: 0.073332:  86%|████████▌ | 43/50 [09:43<02:27, 21.05s/it]
============================================================
Trial 43
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000862
dropout: 0.26301575261418764
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_43 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029460).  Saving model ...
Updating learning rate to 0.0008624166512028265
Validation loss decreased (0.029460 --> 0.027727).  Saving model ...
Updating learning rate to 0.00043120832560141327
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00021560416280070664
Validation loss decreased (0.027727 --> 0.025489).  Saving model ...
Updating learning rate to 0.00010780208140035332
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.390104070017666e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.695052035008833e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_43 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049745626747608185, mae:0.07309852540493011, dtw:Not calculated


                                                                                     [I 2025-10-16 22:52:05,479] Trial 43 finished with value: 0.07309852540493011 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0008624166512028265, 'dropout': 0.26301575261418764}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 30. Best value: 0.073332:  86%|████████▌ | 43/50 [10:01<02:27, 21.05s/it]Best trial: 43. Best value: 0.0730985:  86%|████████▌ | 43/50 [10:01<02:27, 21.05s/it]Best trial: 43. Best value: 0.0730985:  88%|████████▊ | 44/50 [10:01<02:02, 20.36s/it]
============================================================
Trial 44
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000992
dropout: 0.2908466888793985
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_44 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.032639).  Saving model ...
Updating learning rate to 0.0009916404292996052
Validation loss decreased (0.032639 --> 0.025625).  Saving model ...
Updating learning rate to 0.0004958202146498026
Validation loss decreased (0.025625 --> 0.025093).  Saving model ...
Updating learning rate to 0.0002479101073249013
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012395505366245065
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.197752683122533e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_44 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049541983753442764, mae:0.07370822131633759, dtw:Not calculated


                                                                                      [I 2025-10-16 22:52:21,718] Trial 44 finished with value: 0.07370822131633759 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009916404292996052, 'dropout': 0.2908466888793985}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 43. Best value: 0.0730985:  88%|████████▊ | 44/50 [10:18<02:02, 20.36s/it]Best trial: 43. Best value: 0.0730985:  88%|████████▊ | 44/50 [10:18<02:02, 20.36s/it]Best trial: 43. Best value: 0.0730985:  90%|█████████ | 45/50 [10:18<01:35, 19.12s/it]
============================================================
Trial 45
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000907
dropout: 0.2896166619809138
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_45 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026615).  Saving model ...
Updating learning rate to 0.0009066358202105366
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004533179101052683
Validation loss decreased (0.026615 --> 0.025232).  Saving model ...
Updating learning rate to 0.00022665895505263414
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011332947752631707
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.6664738763158536e-05
Validation loss decreased (0.025232 --> 0.024799).  Saving model ...
Updating learning rate to 2.8332369381579268e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4166184690789634e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.083092345394817e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_45 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04927246645092964, mae:0.07382731884717941, dtw:Not calculated


                                                                                      [I 2025-10-16 22:52:45,609] Trial 45 finished with value: 0.07382731884717941 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009066358202105366, 'dropout': 0.2896166619809138}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 43. Best value: 0.0730985:  90%|█████████ | 45/50 [10:42<01:35, 19.12s/it]Best trial: 43. Best value: 0.0730985:  90%|█████████ | 45/50 [10:42<01:35, 19.12s/it]Best trial: 43. Best value: 0.0730985:  92%|█████████▏| 46/50 [10:42<01:22, 20.56s/it]
============================================================
Trial 46
============================================================
seq_len: 48, pred_len: 14
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 64, lr: 0.000023
dropout: 0.2984129218090141
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_46 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.037816).  Saving model ...
Updating learning rate to 2.2792479758116316e-05
Validation loss decreased (0.037816 --> 0.037808).  Saving model ...
Updating learning rate to 1.1396239879058158e-05
Validation loss decreased (0.037808 --> 0.037749).  Saving model ...
Updating learning rate to 5.698119939529079e-06
Validation loss decreased (0.037749 --> 0.037614).  Saving model ...
Updating learning rate to 2.8490599697645395e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4245299848822697e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.122649924411349e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_46 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06485562771558762, mae:0.09963886439800262, dtw:Not calculated


                                                                                      [I 2025-10-16 22:52:51,106] Trial 46 finished with value: 0.09963886439800262 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 2.2792479758116316e-05, 'dropout': 0.2984129218090141}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 43. Best value: 0.0730985:  92%|█████████▏| 46/50 [10:47<01:22, 20.56s/it]Best trial: 43. Best value: 0.0730985:  92%|█████████▏| 46/50 [10:47<01:22, 20.56s/it]Best trial: 43. Best value: 0.0730985:  94%|█████████▍| 47/50 [10:47<00:48, 16.04s/it]
============================================================
Trial 47
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000935
dropout: 0.2597319182572787
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_47 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026753).  Saving model ...
Updating learning rate to 0.0009346531635130104
Validation loss decreased (0.026753 --> 0.025342).  Saving model ...
Updating learning rate to 0.0004673265817565052
Validation loss decreased (0.025342 --> 0.025181).  Saving model ...
Updating learning rate to 0.0002336632908782526
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001168316454391263
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.841582271956315e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_47 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050497014075517654, mae:0.07738426327705383, dtw:Not calculated


                                                                                      [I 2025-10-16 22:53:07,043] Trial 47 finished with value: 0.07738426327705383 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0009346531635130104, 'dropout': 0.2597319182572787}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 43. Best value: 0.0730985:  94%|█████████▍| 47/50 [11:03<00:48, 16.04s/it]Best trial: 43. Best value: 0.0730985:  94%|█████████▍| 47/50 [11:03<00:48, 16.04s/it]Best trial: 43. Best value: 0.0730985:  96%|█████████▌| 48/50 [11:03<00:32, 16.01s/it]
============================================================
Trial 48
============================================================
seq_len: 96, pred_len: 7
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 16, lr: 0.000010
dropout: 0.28591365497129523
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_48 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.036761).  Saving model ...
Updating learning rate to 1.0008095696138605e-05
Validation loss decreased (0.036761 --> 0.036705).  Saving model ...
Updating learning rate to 5.004047848069302e-06
Validation loss decreased (0.036705 --> 0.036600).  Saving model ...
Updating learning rate to 2.502023924034651e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.2510119620173256e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.255059810086628e-07
Validation loss decreased (0.036600 --> 0.036500).  Saving model ...
Updating learning rate to 3.127529905043314e-07
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.563764952521657e-07
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.818824762608285e-08
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_48 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06699079275131226, mae:0.11459216475486755, dtw:Not calculated


                                                                                      [I 2025-10-16 22:53:30,742] Trial 48 finished with value: 0.11459216475486755 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 1.0008095696138605e-05, 'dropout': 0.28591365497129523}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 43. Best value: 0.0730985:  96%|█████████▌| 48/50 [11:27<00:32, 16.01s/it]Best trial: 43. Best value: 0.0730985:  96%|█████████▌| 48/50 [11:27<00:32, 16.01s/it]Best trial: 43. Best value: 0.0730985:  98%|█████████▊| 49/50 [11:27<00:18, 18.31s/it]
============================================================
Trial 49
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 16, lr: 0.000837
dropout: 0.26711061893870025
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_49 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.029006).  Saving model ...
Updating learning rate to 0.0008365367278550434
Validation loss decreased (0.029006 --> 0.025750).  Saving model ...
Updating learning rate to 0.0004182683639275217
Validation loss decreased (0.025750 --> 0.025520).  Saving model ...
Updating learning rate to 0.00020913418196376085
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010456709098188043
Validation loss decreased (0.025520 --> 0.025242).  Saving model ...
Updating learning rate to 5.228354549094021e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.6141772745470107e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.3070886372735053e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_49 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04959440231323242, mae:0.07422633469104767, dtw:Not calculated


                                                                                      [I 2025-10-16 22:53:52,246] Trial 49 finished with value: 0.07422633469104767 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0008365367278550434, 'dropout': 0.26711061893870025}. Best is trial 43 with value: 0.07309852540493011.
Best trial: 43. Best value: 0.0730985:  98%|█████████▊| 49/50 [11:48<00:18, 18.31s/it]Best trial: 43. Best value: 0.0730985:  98%|█████████▊| 49/50 [11:48<00:18, 18.31s/it]Best trial: 43. Best value: 0.0730985: 100%|██████████| 50/50 [11:48<00:00, 19.27s/it]Best trial: 43. Best value: 0.0730985: 100%|██████████| 50/50 [11:48<00:00, 14.17s/it]

======================================================================
Optimization Complete!
======================================================================
Number of finished trials: 50

Best trial:
  Value (MSE): 0.073099

Best hyperparameters:
  seq_len: 48
  pred_len: 1
  expand: 2
  d_model: 32
  n_heads: 4
  e_layers: 1
  d_layers: 2
  batch_size: 16
  learning_rate: 0.0008624166512028265
  dropout: 0.26301575261418764
======================================================================

✅ Best parameters saved to: ./optuna_results/Mamba_Exchange_7_best_params.json
✅ Visualizations saved to ./optuna_results/

======================================================================
✅ Hyperparameter tuning complete!
✅ Best parameters saved to: ./optuna_results/Mamba_Exchange_7_best_params.json
======================================================================

Next steps:
  1. Review visualizations in optuna_results/ folder
  2. Train final model:
     python train_best_model.py --model Mamba --train_epochs 100
======================================================================
[I 2025-10-16 22:54:05,956] A new study created in RDB with name: Mamba_Exchange_14

======================================================================
Starting Optuna Hyperparameter Optimization
======================================================================
Model: Mamba
Dataset: custom
Number of trials: 50
Study name: Mamba_Exchange_14
Storage: sqlite:///optuna_study.db
======================================================================

  0%|          | 0/50 [00:00<?, ?it/s]
============================================================
Trial 0
============================================================
seq_len: 96, pred_len: 14
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000017
dropout: 0.13171877519710265
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_0 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.036405).  Saving model ...
Updating learning rate to 1.6950080316111057e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.475040158055528e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.237520079027764e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_0 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06766027212142944, mae:0.11625654250383377, dtw:Not calculated


                                      [I 2025-10-16 22:54:14,066] Trial 0 finished with value: 0.11625654250383377 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 1, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 1.6950080316111057e-05, 'dropout': 0.13171877519710265}. Best is trial 0 with value: 0.11625654250383377.
  0%|          | 0/50 [00:08<?, ?it/s]Best trial: 0. Best value: 0.116257:   0%|          | 0/50 [00:08<?, ?it/s]Best trial: 0. Best value: 0.116257:   2%|▏         | 1/50 [00:08<06:38,  8.13s/it]
============================================================
Trial 1
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000096
dropout: 0.10525412787011036
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_1 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.037396).  Saving model ...
Updating learning rate to 9.555954018191951e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7779770090959755e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3889885045479878e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_1 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.0645337775349617, mae:0.09865500032901764, dtw:Not calculated


                                                                                   [I 2025-10-16 22:54:17,128] Trial 1 finished with value: 0.09865500032901764 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 9.555954018191951e-05, 'dropout': 0.10525412787011036}. Best is trial 1 with value: 0.09865500032901764.
Best trial: 0. Best value: 0.116257:   2%|▏         | 1/50 [00:11<06:38,  8.13s/it]Best trial: 1. Best value: 0.098655:   2%|▏         | 1/50 [00:11<06:38,  8.13s/it]Best trial: 1. Best value: 0.098655:   4%|▍         | 2/50 [00:11<04:07,  5.15s/it]
============================================================
Trial 2
============================================================
seq_len: 96, pred_len: 7
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000381
dropout: 0.12094893873336815
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_2 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.039339).  Saving model ...
Updating learning rate to 0.0003814874378221004
Validation loss decreased (0.039339 --> 0.038596).  Saving model ...
Updating learning rate to 0.0001907437189110502
Validation loss decreased (0.038596 --> 0.038388).  Saving model ...
Updating learning rate to 9.53718594555251e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.768592972776255e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3842964863881276e-05
Validation loss decreased (0.038388 --> 0.038351).  Saving model ...
Updating learning rate to 1.1921482431940638e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.960741215970319e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.9803706079851595e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_2 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06828029453754425, mae:0.10362828522920609, dtw:Not calculated


                                                                                   [I 2025-10-16 22:54:23,517] Trial 2 finished with value: 0.10362828522920609 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 1, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 0.0003814874378221004, 'dropout': 0.12094893873336815}. Best is trial 1 with value: 0.09865500032901764.
Best trial: 1. Best value: 0.098655:   4%|▍         | 2/50 [00:17<04:07,  5.15s/it]Best trial: 1. Best value: 0.098655:   4%|▍         | 2/50 [00:17<04:07,  5.15s/it]Best trial: 1. Best value: 0.098655:   6%|▌         | 3/50 [00:17<04:28,  5.72s/it]
============================================================
Trial 3
============================================================
seq_len: 96, pred_len: 7
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000029
dropout: 0.1636121589210625
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_3 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.039066).  Saving model ...
Updating learning rate to 2.853776827492645e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4268884137463225e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.134442068731612e-06
Validation loss decreased (0.039066 --> 0.038726).  Saving model ...
Updating learning rate to 3.567221034365806e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.783610517182903e-06
Validation loss decreased (0.038726 --> 0.038506).  Saving model ...
Updating learning rate to 8.918052585914515e-07
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.459026292957258e-07
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.229513146478629e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_3 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06724200397729874, mae:0.1074351817369461, dtw:Not calculated


                                                                                   [I 2025-10-16 22:54:47,447] Trial 3 finished with value: 0.1074351817369461 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 2.853776827492645e-05, 'dropout': 0.1636121589210625}. Best is trial 1 with value: 0.09865500032901764.
Best trial: 1. Best value: 0.098655:   6%|▌         | 3/50 [00:41<04:28,  5.72s/it]Best trial: 1. Best value: 0.098655:   6%|▌         | 3/50 [00:41<04:28,  5.72s/it]Best trial: 1. Best value: 0.098655:   8%|▊         | 4/50 [00:41<09:53, 12.91s/it]
============================================================
Trial 4
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 16, lr: 0.000024
dropout: 0.27748908749075035
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_4 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.034883).  Saving model ...
Updating learning rate to 2.3671026768420086e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.1835513384210043e-05
Validation loss decreased (0.034883 --> 0.031938).  Saving model ...
Updating learning rate to 5.9177566921050214e-06
Validation loss decreased (0.031938 --> 0.031777).  Saving model ...
Updating learning rate to 2.9588783460525107e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4794391730262554e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.397195865131277e-07
Validation loss decreased (0.031777 --> 0.031034).  Saving model ...
Updating learning rate to 3.6985979325656384e-07
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.8492989662828192e-07
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.246494831414096e-08
Validation loss decreased (0.031034 --> 0.030918).  Saving model ...
Updating learning rate to 4.623247415707048e-08
>>>>>> Testing on validation set: trial_4 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.06277771294116974, mae:0.10795877128839493, dtw:Not calculated


                                                                                   [I 2025-10-16 22:55:14,185] Trial 4 finished with value: 0.10795877128839493 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 2.3671026768420086e-05, 'dropout': 0.27748908749075035}. Best is trial 1 with value: 0.09865500032901764.
Best trial: 1. Best value: 0.098655:   8%|▊         | 4/50 [01:08<09:53, 12.91s/it]Best trial: 1. Best value: 0.098655:   8%|▊         | 4/50 [01:08<09:53, 12.91s/it]Best trial: 1. Best value: 0.098655:  10%|█         | 5/50 [01:08<13:25, 17.89s/it]
============================================================
Trial 5
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 2
batch_size: 64, lr: 0.000021
dropout: 0.15297078357060023
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_5 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.037353).  Saving model ...
Updating learning rate to 2.065796448913998e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.032898224456999e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.164491122284995e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_5 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06439430266618729, mae:0.09867642819881439, dtw:Not calculated


                                                                                   [I 2025-10-16 22:55:17,430] Trial 5 finished with value: 0.09867642819881439 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 2.065796448913998e-05, 'dropout': 0.15297078357060023}. Best is trial 1 with value: 0.09865500032901764.
Best trial: 1. Best value: 0.098655:  10%|█         | 5/50 [01:11<13:25, 17.89s/it]Best trial: 1. Best value: 0.098655:  10%|█         | 5/50 [01:11<13:25, 17.89s/it]Best trial: 1. Best value: 0.098655:  12%|█▏        | 6/50 [01:11<09:28, 12.91s/it]
============================================================
Trial 6
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000867
dropout: 0.20137443943532643
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_6 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.028930).  Saving model ...
Updating learning rate to 0.0008672416069410873
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00043362080347054364
Validation loss decreased (0.028930 --> 0.024723).  Saving model ...
Updating learning rate to 0.00021681040173527182
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010840520086763591
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.4202600433817955e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_6 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05151078477501869, mae:0.07738547772169113, dtw:Not calculated


                                                                                   [I 2025-10-16 22:55:33,452] Trial 6 finished with value: 0.07738547772169113 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0008672416069410873, 'dropout': 0.20137443943532643}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 1. Best value: 0.098655:  12%|█▏        | 6/50 [01:27<09:28, 12.91s/it]Best trial: 6. Best value: 0.0773855:  12%|█▏        | 6/50 [01:27<09:28, 12.91s/it]Best trial: 6. Best value: 0.0773855:  14%|█▍        | 7/50 [01:27<09:58, 13.93s/it]
============================================================
Trial 7
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000138
dropout: 0.195431829772055
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_7 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.028031).  Saving model ...
Updating learning rate to 0.00013752522094983075
Validation loss decreased (0.028031 --> 0.026066).  Saving model ...
Updating learning rate to 6.876261047491537e-05
Validation loss decreased (0.026066 --> 0.025602).  Saving model ...
Updating learning rate to 3.438130523745769e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.7190652618728843e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.595326309364422e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_7 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.053577620536088943, mae:0.09003116935491562, dtw:Not calculated


                                                                                    [I 2025-10-16 22:55:41,945] Trial 7 finished with value: 0.09003116935491562 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.00013752522094983075, 'dropout': 0.195431829772055}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 6. Best value: 0.0773855:  14%|█▍        | 7/50 [01:35<09:58, 13.93s/it]Best trial: 6. Best value: 0.0773855:  14%|█▍        | 7/50 [01:36<09:58, 13.93s/it]Best trial: 6. Best value: 0.0773855:  16%|█▌        | 8/50 [01:36<08:32, 12.20s/it]
============================================================
Trial 8
============================================================
seq_len: 48, pred_len: 30
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 1
batch_size: 32, lr: 0.000052
dropout: 0.10759898797898365
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_8 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.038030).  Saving model ...
Updating learning rate to 5.232274710545133e-05
Validation loss decreased (0.038030 --> 0.035995).  Saving model ...
Updating learning rate to 2.6161373552725664e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.3080686776362832e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.540343388181416e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_8 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.06563319265842438, mae:0.10316956043243408, dtw:Not calculated


                                                                                    [I 2025-10-16 22:55:49,014] Trial 8 finished with value: 0.10316956043243408 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 5.232274710545133e-05, 'dropout': 0.10759898797898365}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 6. Best value: 0.0773855:  16%|█▌        | 8/50 [01:43<08:32, 12.20s/it]Best trial: 6. Best value: 0.0773855:  16%|█▌        | 8/50 [01:43<08:32, 12.20s/it]Best trial: 6. Best value: 0.0773855:  18%|█▊        | 9/50 [01:43<07:14, 10.59s/it]
============================================================
Trial 9
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000964
dropout: 0.18389595055881658
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_9 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.030054).  Saving model ...
Updating learning rate to 0.0009636485293142216
Validation loss decreased (0.030054 --> 0.027144).  Saving model ...
Updating learning rate to 0.0004818242646571108
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002409121323285554
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001204560661642777
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_9 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.052886951714754105, mae:0.08154502511024475, dtw:Not calculated


                                                                                    [I 2025-10-16 22:55:56,112] Trial 9 finished with value: 0.08154502511024475 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0009636485293142216, 'dropout': 0.18389595055881658}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 6. Best value: 0.0773855:  18%|█▊        | 9/50 [01:50<07:14, 10.59s/it]Best trial: 6. Best value: 0.0773855:  18%|█▊        | 9/50 [01:50<07:14, 10.59s/it]Best trial: 6. Best value: 0.0773855:  20%|██        | 10/50 [01:50<06:20,  9.52s/it]
============================================================
Trial 10
============================================================
seq_len: 96, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000788
dropout: 0.021573794309979938
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_10 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.038890).  Saving model ...
Updating learning rate to 0.0007881859295063169
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00039409296475315846
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00019704648237657923
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_10 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.07416743785142899, mae:0.10843903571367264, dtw:Not calculated


                                                                                     [I 2025-10-16 22:56:07,035] Trial 10 finished with value: 0.10843903571367264 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0007881859295063169, 'dropout': 0.021573794309979938}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 6. Best value: 0.0773855:  20%|██        | 10/50 [02:01<06:20,  9.52s/it]Best trial: 6. Best value: 0.0773855:  20%|██        | 10/50 [02:01<06:20,  9.52s/it]Best trial: 6. Best value: 0.0773855:  22%|██▏       | 11/50 [02:01<06:27,  9.95s/it]
============================================================
Trial 11
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000960
dropout: 0.23460046148565478
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_11 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.030877).  Saving model ...
Updating learning rate to 0.0009603858719577679
Validation loss decreased (0.030877 --> 0.025451).  Saving model ...
Updating learning rate to 0.00048019293597888397
Validation loss decreased (0.025451 --> 0.025117).  Saving model ...
Updating learning rate to 0.00024009646798944198
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012004823399472099
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.0024116997360496e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_11 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050595443695783615, mae:0.07807992398738861, dtw:Not calculated


                                                                                     [I 2025-10-16 22:56:15,530] Trial 11 finished with value: 0.07807992398738861 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.0009603858719577679, 'dropout': 0.23460046148565478}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 6. Best value: 0.0773855:  22%|██▏       | 11/50 [02:09<06:27,  9.95s/it]Best trial: 6. Best value: 0.0773855:  22%|██▏       | 11/50 [02:09<06:27,  9.95s/it]Best trial: 6. Best value: 0.0773855:  24%|██▍       | 12/50 [02:09<06:01,  9.51s/it]
============================================================
Trial 12
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000351
dropout: 0.2526269110814221
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_12 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.030229).  Saving model ...
Updating learning rate to 0.0003506053530115379
Validation loss decreased (0.030229 --> 0.028823).  Saving model ...
Updating learning rate to 0.00017530267650576895
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.765133825288447e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.382566912644224e-05
Validation loss decreased (0.028823 --> 0.027967).  Saving model ...
Updating learning rate to 2.191283456322112e-05
Validation loss decreased (0.027967 --> 0.027707).  Saving model ...
Updating learning rate to 1.095641728161056e-05
Validation loss decreased (0.027707 --> 0.027440).  Saving model ...
Updating learning rate to 5.47820864080528e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.73910432040264e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.36955216020132e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_12 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05337253212928772, mae:0.08037770539522171, dtw:Not calculated


                                                                                     [I 2025-10-16 22:56:42,127] Trial 12 finished with value: 0.08037770539522171 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0003506053530115379, 'dropout': 0.2526269110814221}. Best is trial 6 with value: 0.07738547772169113.
Best trial: 6. Best value: 0.0773855:  24%|██▍       | 12/50 [02:36<06:01,  9.51s/it]Best trial: 6. Best value: 0.0773855:  24%|██▍       | 12/50 [02:36<06:01,  9.51s/it]Best trial: 6. Best value: 0.0773855:  26%|██▌       | 13/50 [02:36<09:03, 14.69s/it]
============================================================
Trial 13
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000421
dropout: 0.2272722210856118
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_13 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029959).  Saving model ...
Updating learning rate to 0.00042063988313753575
Validation loss decreased (0.029959 --> 0.026664).  Saving model ...
Updating learning rate to 0.00021031994156876788
Validation loss decreased (0.026664 --> 0.025516).  Saving model ...
Updating learning rate to 0.00010515997078438394
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.257998539219197e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.6289992696095985e-05
Validation loss decreased (0.025516 --> 0.024619).  Saving model ...
Updating learning rate to 1.3144996348047992e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.572498174023996e-06
Validation loss decreased (0.024619 --> 0.024348).  Saving model ...
Updating learning rate to 3.286249087011998e-06
Validation loss decreased (0.024348 --> 0.024304).  Saving model ...
Updating learning rate to 1.643124543505999e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.215622717529995e-07
>>>>>> Testing on validation set: trial_13 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04937748983502388, mae:0.0740133672952652, dtw:Not calculated


                                                                                     [I 2025-10-16 22:56:56,008] Trial 13 finished with value: 0.0740133672952652 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00042063988313753575, 'dropout': 0.2272722210856118}. Best is trial 13 with value: 0.0740133672952652.
Best trial: 6. Best value: 0.0773855:  26%|██▌       | 13/50 [02:50<09:03, 14.69s/it]Best trial: 13. Best value: 0.0740134:  26%|██▌       | 13/50 [02:50<09:03, 14.69s/it]Best trial: 13. Best value: 0.0740134:  28%|██▊       | 14/50 [02:50<08:39, 14.44s/it]
============================================================
Trial 14
============================================================
seq_len: 96, pred_len: 30
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000372
dropout: 0.22787168876415048
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_14 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.036624).  Saving model ...
Updating learning rate to 0.00037190447466436163
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018595223733218082
Validation loss decreased (0.036624 --> 0.036157).  Saving model ...
Updating learning rate to 9.297611866609041e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.6488059333045204e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.3244029666522602e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_14 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07436720281839371, mae:0.1157316043972969, dtw:Not calculated


                                                                                      [I 2025-10-16 22:57:12,285] Trial 14 finished with value: 0.1157316043972969 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00037190447466436163, 'dropout': 0.22787168876415048}. Best is trial 13 with value: 0.0740133672952652.
Best trial: 13. Best value: 0.0740134:  28%|██▊       | 14/50 [03:06<08:39, 14.44s/it]Best trial: 13. Best value: 0.0740134:  28%|██▊       | 14/50 [03:06<08:39, 14.44s/it]Best trial: 13. Best value: 0.0740134:  30%|███       | 15/50 [03:06<08:44, 14.99s/it]
============================================================
Trial 15
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000190
dropout: 0.2972386400728122
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_15 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029172).  Saving model ...
Updating learning rate to 0.000190457229301115
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.52286146505575e-05
Validation loss decreased (0.029172 --> 0.027363).  Saving model ...
Updating learning rate to 4.761430732527875e-05
Validation loss decreased (0.027363 --> 0.025807).  Saving model ...
Updating learning rate to 2.3807153662639376e-05
Validation loss decreased (0.025807 --> 0.025771).  Saving model ...
Updating learning rate to 1.1903576831319688e-05
Validation loss decreased (0.025771 --> 0.025496).  Saving model ...
Updating learning rate to 5.951788415659844e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.975894207829922e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.487947103914961e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_15 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.052007876336574554, mae:0.07908625900745392, dtw:Not calculated


                                                                                      [I 2025-10-16 22:57:24,921] Trial 15 finished with value: 0.07908625900745392 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.000190457229301115, 'dropout': 0.2972386400728122}. Best is trial 13 with value: 0.0740133672952652.
Best trial: 13. Best value: 0.0740134:  30%|███       | 15/50 [03:18<08:44, 14.99s/it]Best trial: 13. Best value: 0.0740134:  30%|███       | 15/50 [03:18<08:44, 14.99s/it]Best trial: 13. Best value: 0.0740134:  32%|███▏      | 16/50 [03:18<08:05, 14.28s/it]
============================================================
Trial 16
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000514
dropout: 0.21569043765853188
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_16 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026960).  Saving model ...
Updating learning rate to 0.0005136260094450086
Validation loss decreased (0.026960 --> 0.026644).  Saving model ...
Updating learning rate to 0.0002568130047225043
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012840650236125214
Validation loss decreased (0.026644 --> 0.024982).  Saving model ...
Updating learning rate to 6.420325118062607e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.2101625590313036e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.6050812795156518e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_16 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04963470250368118, mae:0.0719204694032669, dtw:Not calculated


                                                                                      [I 2025-10-16 22:57:43,686] Trial 16 finished with value: 0.0719204694032669 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0005136260094450086, 'dropout': 0.21569043765853188}. Best is trial 16 with value: 0.0719204694032669.
Best trial: 13. Best value: 0.0740134:  32%|███▏      | 16/50 [03:37<08:05, 14.28s/it]Best trial: 16. Best value: 0.0719205:  32%|███▏      | 16/50 [03:37<08:05, 14.28s/it]Best trial: 16. Best value: 0.0719205:  34%|███▍      | 17/50 [03:37<08:35, 15.63s/it]
============================================================
Trial 17
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 32, lr: 0.000235
dropout: 0.05252223613286254
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_17 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.025126).  Saving model ...
Updating learning rate to 0.00023488600114387957
Validation loss decreased (0.025126 --> 0.024764).  Saving model ...
Updating learning rate to 0.00011744300057193979
Validation loss decreased (0.024764 --> 0.024665).  Saving model ...
Updating learning rate to 5.8721500285969894e-05
Validation loss decreased (0.024665 --> 0.023730).  Saving model ...
Updating learning rate to 2.9360750142984947e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4680375071492473e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.340187535746237e-06
Validation loss decreased (0.023730 --> 0.023405).  Saving model ...
Updating learning rate to 3.6700937678731183e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.8350468839365592e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.175234419682796e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_17 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04995022341609001, mae:0.07325895130634308, dtw:Not calculated


                                                                                      [I 2025-10-16 22:57:57,647] Trial 17 finished with value: 0.07325895130634308 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 0.00023488600114387957, 'dropout': 0.05252223613286254}. Best is trial 16 with value: 0.0719204694032669.
Best trial: 16. Best value: 0.0719205:  34%|███▍      | 17/50 [03:51<08:35, 15.63s/it]Best trial: 16. Best value: 0.0719205:  34%|███▍      | 17/50 [03:51<08:35, 15.63s/it]Best trial: 16. Best value: 0.0719205:  36%|███▌      | 18/50 [03:51<08:04, 15.13s/it]
============================================================
Trial 18
============================================================
seq_len: 48, pred_len: 30
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000208
dropout: 0.04578204712792089
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_18 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037626).  Saving model ...
Updating learning rate to 0.00020814804330499434
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010407402165249717
Validation loss decreased (0.037626 --> 0.036873).  Saving model ...
Updating learning rate to 5.2037010826248586e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.6018505413124293e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.3009252706562146e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_18 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07287681847810745, mae:0.10881824791431427, dtw:Not calculated


                                                                                      [I 2025-10-16 22:58:13,771] Trial 18 finished with value: 0.10881824791431427 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00020814804330499434, 'dropout': 0.04578204712792089}. Best is trial 16 with value: 0.0719204694032669.
Best trial: 16. Best value: 0.0719205:  36%|███▌      | 18/50 [04:07<08:04, 15.13s/it]Best trial: 16. Best value: 0.0719205:  36%|███▌      | 18/50 [04:07<08:04, 15.13s/it]Best trial: 16. Best value: 0.0719205:  38%|███▊      | 19/50 [04:07<07:58, 15.43s/it]
============================================================
Trial 19
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 2
batch_size: 64, lr: 0.000081
dropout: 0.07571596423910953
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_19 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.038495).  Saving model ...
Updating learning rate to 8.134754042740414e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.067377021370207e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.0336885106851035e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_19 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06453371047973633, mae:0.09914474189281464, dtw:Not calculated


                                                                                      [I 2025-10-16 22:58:17,108] Trial 19 finished with value: 0.09914474189281464 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 8.134754042740414e-05, 'dropout': 0.07571596423910953}. Best is trial 16 with value: 0.0719204694032669.
Best trial: 16. Best value: 0.0719205:  38%|███▊      | 19/50 [04:11<07:58, 15.43s/it]Best trial: 16. Best value: 0.0719205:  38%|███▊      | 19/50 [04:11<07:58, 15.43s/it]Best trial: 16. Best value: 0.0719205:  40%|████      | 20/50 [04:11<05:53, 11.80s/it]
============================================================
Trial 20
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000551
dropout: 0.05561617345359868
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_20 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029283).  Saving model ...
Updating learning rate to 0.0005507127388623045
Validation loss decreased (0.029283 --> 0.025922).  Saving model ...
Updating learning rate to 0.00027535636943115227
Validation loss decreased (0.025922 --> 0.025429).  Saving model ...
Updating learning rate to 0.00013767818471557613
Validation loss decreased (0.025429 --> 0.024269).  Saving model ...
Updating learning rate to 6.883909235778807e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.441954617889403e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.7209773089447017e-05
Validation loss decreased (0.024269 --> 0.023858).  Saving model ...
Updating learning rate to 8.604886544723508e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.302443272361754e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.151221636180877e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_20 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049156323075294495, mae:0.070341557264328, dtw:Not calculated


                                                                                      [I 2025-10-16 22:58:31,189] Trial 20 finished with value: 0.070341557264328 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0005507127388623045, 'dropout': 0.05561617345359868}. Best is trial 20 with value: 0.070341557264328.
Best trial: 16. Best value: 0.0719205:  40%|████      | 20/50 [04:25<05:53, 11.80s/it]Best trial: 20. Best value: 0.0703416:  40%|████      | 20/50 [04:25<05:53, 11.80s/it]Best trial: 20. Best value: 0.0703416:  42%|████▏     | 21/50 [04:25<06:01, 12.48s/it]
============================================================
Trial 21
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000558
dropout: 0.007168249314464831
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_21 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029082).  Saving model ...
Updating learning rate to 0.0005575214750917238
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002787607375458619
Validation loss decreased (0.029082 --> 0.025941).  Saving model ...
Updating learning rate to 0.00013938036877293096
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.969018438646548e-05
Validation loss decreased (0.025941 --> 0.025030).  Saving model ...
Updating learning rate to 3.484509219323274e-05
Validation loss decreased (0.025030 --> 0.024033).  Saving model ...
Updating learning rate to 1.742254609661637e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.711273048308185e-06
Validation loss decreased (0.024033 --> 0.023745).  Saving model ...
Updating learning rate to 4.3556365241540924e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.1778182620770462e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.0889091310385231e-06
>>>>>> Testing on validation set: trial_21 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049241188913583755, mae:0.07199005782604218, dtw:Not calculated


                                                                                      [I 2025-10-16 22:58:45,305] Trial 21 finished with value: 0.07199005782604218 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0005575214750917238, 'dropout': 0.007168249314464831}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  42%|████▏     | 21/50 [04:39<06:01, 12.48s/it]Best trial: 20. Best value: 0.0703416:  42%|████▏     | 21/50 [04:39<06:01, 12.48s/it]Best trial: 20. Best value: 0.0703416:  44%|████▍     | 22/50 [04:39<06:03, 12.98s/it]
============================================================
Trial 22
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000565
dropout: 0.026187530729775536
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_22 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.025595).  Saving model ...
Updating learning rate to 0.0005651796662911049
Validation loss decreased (0.025595 --> 0.025059).  Saving model ...
Updating learning rate to 0.00028258983314555247
Validation loss decreased (0.025059 --> 0.025035).  Saving model ...
Updating learning rate to 0.00014129491657277623
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.064745828638812e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.532372914319406e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_22 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050755467265844345, mae:0.07389911264181137, dtw:Not calculated


                                                                                      [I 2025-10-16 22:58:53,849] Trial 22 finished with value: 0.07389911264181137 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0005651796662911049, 'dropout': 0.026187530729775536}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  44%|████▍     | 22/50 [04:47<06:03, 12.98s/it]Best trial: 20. Best value: 0.0703416:  44%|████▍     | 22/50 [04:47<06:03, 12.98s/it]Best trial: 20. Best value: 0.0703416:  46%|████▌     | 23/50 [04:47<05:14, 11.64s/it]
============================================================
Trial 23
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000680
dropout: 0.002445196551479704
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_23 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026573).  Saving model ...
Updating learning rate to 0.0006795466337230016
Validation loss decreased (0.026573 --> 0.024516).  Saving model ...
Updating learning rate to 0.0003397733168615008
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001698866584307504
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.49433292153752e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_23 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05026093125343323, mae:0.07357482612133026, dtw:Not calculated


                                                                                      [I 2025-10-16 22:59:01,114] Trial 23 finished with value: 0.07357482612133026 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0006795466337230016, 'dropout': 0.002445196551479704}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  46%|████▌     | 23/50 [04:55<05:14, 11.64s/it]Best trial: 20. Best value: 0.0703416:  46%|████▌     | 23/50 [04:55<05:14, 11.64s/it]Best trial: 20. Best value: 0.0703416:  48%|████▊     | 24/50 [04:55<04:28, 10.33s/it]
============================================================
Trial 24
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000011
dropout: 0.07759527822923379
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_24 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.033640).  Saving model ...
Updating learning rate to 1.0664801735955304e-05
Validation loss decreased (0.033640 --> 0.032174).  Saving model ...
Updating learning rate to 5.332400867977652e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.666200433988826e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.333100216994413e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_24 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.061303652822971344, mae:0.09491308033466339, dtw:Not calculated


                                                                                      [I 2025-10-16 22:59:08,336] Trial 24 finished with value: 0.09491308033466339 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 1.0664801735955304e-05, 'dropout': 0.07759527822923379}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  48%|████▊     | 24/50 [05:02<04:28, 10.33s/it]Best trial: 20. Best value: 0.0703416:  48%|████▊     | 24/50 [05:02<04:28, 10.33s/it]Best trial: 20. Best value: 0.0703416:  50%|█████     | 25/50 [05:02<03:54,  9.40s/it]
============================================================
Trial 25
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000545
dropout: 0.000378869778120002
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_25 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.025690).  Saving model ...
Updating learning rate to 0.0005445061185089705
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00027225305925448524
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013612652962724262
Validation loss decreased (0.025690 --> 0.025206).  Saving model ...
Updating learning rate to 6.806326481362131e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.4031632406810655e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.7015816203405328e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_25 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04982452467083931, mae:0.07251806557178497, dtw:Not calculated


                                                                                      [I 2025-10-16 22:59:27,354] Trial 25 finished with value: 0.07251806557178497 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0005445061185089705, 'dropout': 0.000378869778120002}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  50%|█████     | 25/50 [05:21<03:54,  9.40s/it]Best trial: 20. Best value: 0.0703416:  50%|█████     | 25/50 [05:21<03:54,  9.40s/it]Best trial: 20. Best value: 0.0703416:  52%|█████▏    | 26/50 [05:21<04:54, 12.28s/it]
============================================================
Trial 26
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000278
dropout: 0.08379772862817475
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_26 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.028404).  Saving model ...
Updating learning rate to 0.00027787839245840444
Validation loss decreased (0.028404 --> 0.025437).  Saving model ...
Updating learning rate to 0.00013893919622920222
Validation loss decreased (0.025437 --> 0.023945).  Saving model ...
Updating learning rate to 6.946959811460111e-05
Validation loss decreased (0.023945 --> 0.023685).  Saving model ...
Updating learning rate to 3.4734799057300555e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.7367399528650277e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.683699764325139e-06
Validation loss decreased (0.023685 --> 0.023591).  Saving model ...
Updating learning rate to 4.341849882162569e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.1709249410812847e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.0854624705406423e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_26 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04870020970702171, mae:0.07182317227125168, dtw:Not calculated


                                                                                      [I 2025-10-16 22:59:41,432] Trial 26 finished with value: 0.07182317227125168 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.00027787839245840444, 'dropout': 0.08379772862817475}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  52%|█████▏    | 26/50 [05:35<04:54, 12.28s/it]Best trial: 20. Best value: 0.0703416:  52%|█████▏    | 26/50 [05:35<04:54, 12.28s/it]Best trial: 20. Best value: 0.0703416:  54%|█████▍    | 27/50 [05:35<04:54, 12.82s/it]
============================================================
Trial 27
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000276
dropout: 0.07889827972571062
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_27 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.039987).  Saving model ...
Updating learning rate to 0.0002758159759626864
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001379079879813432
Validation loss decreased (0.039987 --> 0.039874).  Saving model ...
Updating learning rate to 6.89539939906716e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.44769969953358e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.72384984976679e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_27 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.07097915560007095, mae:0.10512518137693405, dtw:Not calculated


                                                                                      [I 2025-10-16 22:59:49,943] Trial 27 finished with value: 0.10512518137693405 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0002758159759626864, 'dropout': 0.07889827972571062}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  54%|█████▍    | 27/50 [05:43<04:54, 12.82s/it]Best trial: 20. Best value: 0.0703416:  54%|█████▍    | 27/50 [05:44<04:54, 12.82s/it]Best trial: 20. Best value: 0.0703416:  56%|█████▌    | 28/50 [05:44<04:13, 11.53s/it]
============================================================
Trial 28
============================================================
seq_len: 48, pred_len: 30
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000177
dropout: 0.05025279427930014
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_28 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037477).  Saving model ...
Updating learning rate to 0.00017731201091048554
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.865600545524277e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.4328002727621385e-05
Validation loss decreased (0.037477 --> 0.037131).  Saving model ...
Updating learning rate to 2.2164001363810693e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.1082000681905346e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.541000340952673e-06
Validation loss decreased (0.037131 --> 0.037055).  Saving model ...
Updating learning rate to 2.7705001704763366e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.3852500852381683e-06
Validation loss decreased (0.037055 --> 0.037044).  Saving model ...
Updating learning rate to 6.926250426190841e-07
Validation loss decreased (0.037044 --> 0.036882).  Saving model ...
Updating learning rate to 3.4631252130954207e-07
>>>>>> Testing on validation set: trial_28 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.0743740051984787, mae:0.1095481887459755, dtw:Not calculated


                                                                                      [I 2025-10-16 23:00:16,721] Trial 28 finished with value: 0.1095481887459755 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00017731201091048554, 'dropout': 0.05025279427930014}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  56%|█████▌    | 28/50 [06:10<04:13, 11.53s/it]Best trial: 20. Best value: 0.0703416:  56%|█████▌    | 28/50 [06:10<04:13, 11.53s/it]Best trial: 20. Best value: 0.0703416:  58%|█████▊    | 29/50 [06:10<05:38, 16.10s/it]
============================================================
Trial 29
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 64, lr: 0.000306
dropout: 0.0939938525611088
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_29 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.039035).  Saving model ...
Updating learning rate to 0.00030563618213833444
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015281809106916722
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.640904553458361e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_29 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06610441207885742, mae:0.09897374361753464, dtw:Not calculated


                                                                                      [I 2025-10-16 23:00:20,027] Trial 29 finished with value: 0.09897374361753464 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 0.00030563618213833444, 'dropout': 0.0939938525611088}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  58%|█████▊    | 29/50 [06:14<05:38, 16.10s/it]Best trial: 20. Best value: 0.0703416:  58%|█████▊    | 29/50 [06:14<05:38, 16.10s/it]Best trial: 20. Best value: 0.0703416:  60%|██████    | 30/50 [06:14<04:05, 12.26s/it]
============================================================
Trial 30
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000138
dropout: 0.1396975545461726
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_30 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029968).  Saving model ...
Updating learning rate to 0.0001384175431715725
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.920877158578625e-05
Validation loss decreased (0.029968 --> 0.025894).  Saving model ...
Updating learning rate to 3.4604385792893125e-05
Validation loss decreased (0.025894 --> 0.025262).  Saving model ...
Updating learning rate to 1.7302192896446562e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.651096448223281e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.325548224111641e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_30 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05171205848455429, mae:0.07929237186908722, dtw:Not calculated


                                                                                      [I 2025-10-16 23:00:30,112] Trial 30 finished with value: 0.07929237186908722 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0001384175431715725, 'dropout': 0.1396975545461726}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  60%|██████    | 30/50 [06:24<04:05, 12.26s/it]Best trial: 20. Best value: 0.0703416:  60%|██████    | 30/50 [06:24<04:05, 12.26s/it]Best trial: 20. Best value: 0.0703416:  62%|██████▏   | 31/50 [06:24<03:40, 11.61s/it]
============================================================
Trial 31
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000521
dropout: 0.04404879177044514
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_31 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.024888).  Saving model ...
Updating learning rate to 0.0005213141555765289
Validation loss decreased (0.024888 --> 0.024575).  Saving model ...
Updating learning rate to 0.00026065707778826446
Validation loss decreased (0.024575 --> 0.023874).  Saving model ...
Updating learning rate to 0.00013032853889413223
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.516426944706611e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.258213472353306e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_31 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0516577884554863, mae:0.07344827055931091, dtw:Not calculated


                                                                                      [I 2025-10-16 23:00:38,732] Trial 31 finished with value: 0.07344827055931091 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0005213141555765289, 'dropout': 0.04404879177044514}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  62%|██████▏   | 31/50 [06:32<03:40, 11.61s/it]Best trial: 20. Best value: 0.0703416:  62%|██████▏   | 31/50 [06:32<03:40, 11.61s/it]Best trial: 20. Best value: 0.0703416:  64%|██████▍   | 32/50 [06:32<03:12, 10.72s/it]
============================================================
Trial 32
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000476
dropout: 0.014401268619261887
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_32 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.025987).  Saving model ...
Updating learning rate to 0.00047644560379067104
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00023822280189533552
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011911140094766776
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_32 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05048510059714317, mae:0.07535851746797562, dtw:Not calculated


                                                                                      [I 2025-10-16 23:00:44,560] Trial 32 finished with value: 0.07535851746797562 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.00047644560379067104, 'dropout': 0.014401268619261887}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  64%|██████▍   | 32/50 [06:38<03:12, 10.72s/it]Best trial: 20. Best value: 0.0703416:  64%|██████▍   | 32/50 [06:38<03:12, 10.72s/it]Best trial: 20. Best value: 0.0703416:  66%|██████▌   | 33/50 [06:38<02:37,  9.25s/it]
============================================================
Trial 33
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000705
dropout: 0.12307461532808024
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_33 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.025743).  Saving model ...
Updating learning rate to 0.0007054387885155116
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003527193942577558
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001763596971288779
Validation loss decreased (0.025743 --> 0.024378).  Saving model ...
Updating learning rate to 8.817984856443895e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.408992428221948e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.204496214110974e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_33 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049827080219984055, mae:0.07346164435148239, dtw:Not calculated


                                                                                      [I 2025-10-16 23:00:54,568] Trial 33 finished with value: 0.07346164435148239 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0007054387885155116, 'dropout': 0.12307461532808024}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  66%|██████▌   | 33/50 [06:48<02:37,  9.25s/it]Best trial: 20. Best value: 0.0703416:  66%|██████▌   | 33/50 [06:48<02:37,  9.25s/it]Best trial: 20. Best value: 0.0703416:  68%|██████▊   | 34/50 [06:48<02:31,  9.48s/it]
============================================================
Trial 34
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000621
dropout: 0.0627419412092727
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_34 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.039293).  Saving model ...
Updating learning rate to 0.000620516474137526
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000310258237068763
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001551291185343815
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_34 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06657073646783829, mae:0.09659086912870407, dtw:Not calculated


                                                                                      [I 2025-10-16 23:01:00,409] Trial 34 finished with value: 0.09659086912870407 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.000620516474137526, 'dropout': 0.0627419412092727}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  68%|██████▊   | 34/50 [06:54<02:31,  9.48s/it]Best trial: 20. Best value: 0.0703416:  68%|██████▊   | 34/50 [06:54<02:31,  9.48s/it]Best trial: 20. Best value: 0.0703416:  70%|███████   | 35/50 [06:54<02:05,  8.39s/it]
============================================================
Trial 35
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 1
batch_size: 64, lr: 0.000311
dropout: 0.031035040080324632
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_35 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026962).  Saving model ...
Updating learning rate to 0.0003113759906399215
Validation loss decreased (0.026962 --> 0.026860).  Saving model ...
Updating learning rate to 0.00015568799531996076
Validation loss decreased (0.026860 --> 0.022950).  Saving model ...
Updating learning rate to 7.784399765998038e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.892199882999019e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.9460999414995095e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_35 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.050935931503772736, mae:0.07572775334119797, dtw:Not calculated


                                                                                      [I 2025-10-16 23:01:05,186] Trial 35 finished with value: 0.07572775334119797 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 0.0003113759906399215, 'dropout': 0.031035040080324632}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  70%|███████   | 35/50 [06:59<02:05,  8.39s/it]Best trial: 20. Best value: 0.0703416:  70%|███████   | 35/50 [06:59<02:05,  8.39s/it]Best trial: 20. Best value: 0.0703416:  72%|███████▏  | 36/50 [06:59<01:42,  7.30s/it]
============================================================
Trial 36
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000441
dropout: 0.16991774462719306
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_36 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.038445).  Saving model ...
Updating learning rate to 0.00044066915384005515
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022033457692002757
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011016728846001379
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_36 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06612934917211533, mae:0.0989219918847084, dtw:Not calculated


                                                                                      [I 2025-10-16 23:01:10,948] Trial 36 finished with value: 0.0989219918847084 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.00044066915384005515, 'dropout': 0.16991774462719306}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  72%|███████▏  | 36/50 [07:04<01:42,  7.30s/it]Best trial: 20. Best value: 0.0703416:  72%|███████▏  | 36/50 [07:05<01:42,  7.30s/it]Best trial: 20. Best value: 0.0703416:  74%|███████▍  | 37/50 [07:05<01:28,  6.84s/it]
============================================================
Trial 37
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000074
dropout: 0.08905302749466334
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_37 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.032860).  Saving model ...
Updating learning rate to 7.394646363224577e-05
Validation loss decreased (0.032860 --> 0.032242).  Saving model ...
Updating learning rate to 3.6973231816122884e-05
Validation loss decreased (0.032242 --> 0.029523).  Saving model ...
Updating learning rate to 1.8486615908061442e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.243307954030721e-06
Validation loss decreased (0.029523 --> 0.029448).  Saving model ...
Updating learning rate to 4.6216539770153605e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.3108269885076802e-06
Validation loss decreased (0.029448 --> 0.029139).  Saving model ...
Updating learning rate to 1.1554134942538401e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.777067471269201e-07
Validation loss decreased (0.029139 --> 0.028698).  Saving model ...
Updating learning rate to 2.8885337356346003e-07
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.4442668678173002e-07
>>>>>> Testing on validation set: trial_37 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05606912076473236, mae:0.09340052306652069, dtw:Not calculated


                                                                                      [I 2025-10-16 23:01:37,542] Trial 37 finished with value: 0.09340052306652069 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 7.394646363224577e-05, 'dropout': 0.08905302749466334}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  74%|███████▍  | 37/50 [07:31<01:28,  6.84s/it]Best trial: 20. Best value: 0.0703416:  74%|███████▍  | 37/50 [07:31<01:28,  6.84s/it]Best trial: 20. Best value: 0.0703416:  76%|███████▌  | 38/50 [07:31<02:33, 12.77s/it]
============================================================
Trial 38
============================================================
seq_len: 48, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 64, lr: 0.000044
dropout: 0.10805876927943901
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_38 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.037579).  Saving model ...
Updating learning rate to 4.438988474247917e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.2194942371239584e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.1097471185619792e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_38 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06411430239677429, mae:0.09825744479894638, dtw:Not calculated


                                                                                      [I 2025-10-16 23:01:40,889] Trial 38 finished with value: 0.09825744479894638 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 4.438988474247917e-05, 'dropout': 0.10805876927943901}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  76%|███████▌  | 38/50 [07:34<02:33, 12.77s/it]Best trial: 20. Best value: 0.0703416:  76%|███████▌  | 38/50 [07:34<02:33, 12.77s/it]Best trial: 20. Best value: 0.0703416:  78%|███████▊  | 39/50 [07:34<01:49,  9.94s/it]
============================================================
Trial 39
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 32, lr: 0.000256
dropout: 0.14481627532433286
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_39 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.025194).  Saving model ...
Updating learning rate to 0.0002556340979384369
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012781704896921845
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.390852448460923e-05
Validation loss decreased (0.025194 --> 0.024221).  Saving model ...
Updating learning rate to 3.195426224230461e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5977131121152306e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.988565560576153e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_39 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0501072034239769, mae:0.07633569091558456, dtw:Not calculated


                                                                                      [I 2025-10-16 23:01:50,726] Trial 39 finished with value: 0.07633569091558456 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 32, 'learning_rate': 0.0002556340979384369, 'dropout': 0.14481627532433286}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  78%|███████▊  | 39/50 [07:44<01:49,  9.94s/it]Best trial: 20. Best value: 0.0703416:  78%|███████▊  | 39/50 [07:44<01:49,  9.94s/it]Best trial: 20. Best value: 0.0703416:  80%|████████  | 40/50 [07:44<01:39,  9.91s/it]
============================================================
Trial 40
============================================================
seq_len: 48, pred_len: 1
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000142
dropout: 0.06449537342058136
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_40 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.031953).  Saving model ...
Updating learning rate to 0.0001419754943550058
Validation loss decreased (0.031953 --> 0.027306).  Saving model ...
Updating learning rate to 7.09877471775029e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.549387358875145e-05
Validation loss decreased (0.027306 --> 0.026696).  Saving model ...
Updating learning rate to 1.7746936794375724e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.873468397187862e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.436734198593931e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_40 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.052875760942697525, mae:0.07961301505565643, dtw:Not calculated


                                                                                      [I 2025-10-16 23:02:09,523] Trial 40 finished with value: 0.07961301505565643 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0001419754943550058, 'dropout': 0.06449537342058136}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  80%|████████  | 40/50 [08:03<01:39,  9.91s/it]Best trial: 20. Best value: 0.0703416:  80%|████████  | 40/50 [08:03<01:39,  9.91s/it]Best trial: 20. Best value: 0.0703416:  82%|████████▏ | 41/50 [08:03<01:53, 12.57s/it]
============================================================
Trial 41
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000567
dropout: 0.003350079198119086
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_41 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.026318).  Saving model ...
Updating learning rate to 0.0005668457761098971
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00028342288805494856
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00014171144402747428
Validation loss decreased (0.026318 --> 0.026153).  Saving model ...
Updating learning rate to 7.085572201373714e-05
Validation loss decreased (0.026153 --> 0.025352).  Saving model ...
Updating learning rate to 3.542786100686857e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.7713930503434285e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.856965251717142e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_41 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04975634440779686, mae:0.07042206823825836, dtw:Not calculated


                                                                                      [I 2025-10-16 23:02:31,107] Trial 41 finished with value: 0.07042206823825836 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0005668457761098971, 'dropout': 0.003350079198119086}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  82%|████████▏ | 41/50 [08:25<01:53, 12.57s/it]Best trial: 20. Best value: 0.0703416:  82%|████████▏ | 41/50 [08:25<01:53, 12.57s/it]Best trial: 20. Best value: 0.0703416:  84%|████████▍ | 42/50 [08:25<02:02, 15.28s/it]
============================================================
Trial 42
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000736
dropout: 0.013027853169550766
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_42 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.039907).  Saving model ...
Updating learning rate to 0.0007357965247703189
Validation loss decreased (0.039907 --> 0.027323).  Saving model ...
Updating learning rate to 0.00036789826238515946
Validation loss decreased (0.027323 --> 0.027226).  Saving model ...
Updating learning rate to 0.00018394913119257973
Validation loss decreased (0.027226 --> 0.025205).  Saving model ...
Updating learning rate to 9.197456559628987e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.598728279814493e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.2993641399072467e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_42 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.0501009076833725, mae:0.07223382592201233, dtw:Not calculated


                                                                                      [I 2025-10-16 23:02:50,102] Trial 42 finished with value: 0.07223382592201233 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0007357965247703189, 'dropout': 0.013027853169550766}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  84%|████████▍ | 42/50 [08:44<02:02, 15.28s/it]Best trial: 20. Best value: 0.0703416:  84%|████████▍ | 42/50 [08:44<02:02, 15.28s/it]Best trial: 20. Best value: 0.0703416:  86%|████████▌ | 43/50 [08:44<01:54, 16.39s/it]
============================================================
Trial 43
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000397
dropout: 0.037252835330330004
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_43 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.025843).  Saving model ...
Updating learning rate to 0.00039691454806919674
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00019845727403459837
Validation loss decreased (0.025843 --> 0.025106).  Saving model ...
Updating learning rate to 9.922863701729919e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.961431850864959e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.4807159254324797e-05
Validation loss decreased (0.025106 --> 0.024616).  Saving model ...
Updating learning rate to 1.2403579627162398e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.201789813581199e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.1008949067905996e-06
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_43 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.048779863864183426, mae:0.07055023312568665, dtw:Not calculated


                                                                                      [I 2025-10-16 23:03:14,335] Trial 43 finished with value: 0.07055023312568665 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00039691454806919674, 'dropout': 0.037252835330330004}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  86%|████████▌ | 43/50 [09:08<01:54, 16.39s/it]Best trial: 20. Best value: 0.0703416:  86%|████████▌ | 43/50 [09:08<01:54, 16.39s/it]Best trial: 20. Best value: 0.0703416:  88%|████████▊ | 44/50 [09:08<01:52, 18.75s/it]
============================================================
Trial 44
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000392
dropout: 0.03524505414522364
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_44 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.028908).  Saving model ...
Updating learning rate to 0.00039225434456064594
Validation loss decreased (0.028908 --> 0.025553).  Saving model ...
Updating learning rate to 0.00019612717228032297
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.806358614016149e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.903179307008074e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_44 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05095963180065155, mae:0.07542653381824493, dtw:Not calculated


                                                                                      [I 2025-10-16 23:03:27,913] Trial 44 finished with value: 0.07542653381824493 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.00039225434456064594, 'dropout': 0.03524505414522364}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  88%|████████▊ | 44/50 [09:21<01:52, 18.75s/it]Best trial: 20. Best value: 0.0703416:  88%|████████▊ | 44/50 [09:21<01:52, 18.75s/it]Best trial: 20. Best value: 0.0703416:  90%|█████████ | 45/50 [09:21<01:25, 17.19s/it]
============================================================
Trial 45
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000988
dropout: 0.20813366135029282
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_45 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.032227).  Saving model ...
Updating learning rate to 0.0009875712393954904
Validation loss decreased (0.032227 --> 0.028732).  Saving model ...
Updating learning rate to 0.0004937856196977452
Validation loss decreased (0.028732 --> 0.025742).  Saving model ...
Updating learning rate to 0.0002468928098488726
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001234464049244363
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.172320246221815e-05
Validation loss decreased (0.025742 --> 0.025659).  Saving model ...
Updating learning rate to 3.0861601231109076e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5430800615554538e-05
Validation loss decreased (0.025659 --> 0.025639).  Saving model ...
Updating learning rate to 7.715400307777269e-06
Validation loss decreased (0.025639 --> 0.025278).  Saving model ...
Updating learning rate to 3.8577001538886345e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.9288500769443173e-06
>>>>>> Testing on validation set: trial_45 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049540095031261444, mae:0.07035250216722488, dtw:Not calculated


                                                                                      [I 2025-10-16 23:03:54,851] Trial 45 finished with value: 0.07035250216722488 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0009875712393954904, 'dropout': 0.20813366135029282}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  90%|█████████ | 45/50 [09:48<01:25, 17.19s/it]Best trial: 20. Best value: 0.0703416:  90%|█████████ | 45/50 [09:48<01:25, 17.19s/it]Best trial: 20. Best value: 0.0703416:  92%|█████████▏| 46/50 [09:48<01:20, 20.12s/it]
============================================================
Trial 46
============================================================
seq_len: 96, pred_len: 30
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000963
dropout: 0.05990249764815768
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_46 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.037194).  Saving model ...
Updating learning rate to 0.0009626101771349054
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004813050885674527
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024065254428372635
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_46 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07216694951057434, mae:0.11435441672801971, dtw:Not calculated


                                                                                      [I 2025-10-16 23:04:05,873] Trial 46 finished with value: 0.11435441672801971 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0009626101771349054, 'dropout': 0.05990249764815768}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  92%|█████████▏| 46/50 [09:59<01:20, 20.12s/it]Best trial: 20. Best value: 0.0703416:  92%|█████████▏| 46/50 [09:59<01:20, 20.12s/it]Best trial: 20. Best value: 0.0703416:  94%|█████████▍| 47/50 [09:59<00:52, 17.39s/it]
============================================================
Trial 47
============================================================
seq_len: 48, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000858
dropout: 0.03424471217924429
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_47 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 1
============================================================
train 4625
val 259
test 261
Validation loss decreased (inf --> 0.029759).  Saving model ...
Updating learning rate to 0.000857940067299971
Validation loss decreased (0.029759 --> 0.025480).  Saving model ...
Updating learning rate to 0.0004289700336499855
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00021448501682499275
Validation loss decreased (0.025480 --> 0.025113).  Saving model ...
Updating learning rate to 0.00010724250841249638
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.362125420624819e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.6810627103124094e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_47 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.049294788390398026, mae:0.07131721079349518, dtw:Not calculated


                                                                                      [I 2025-10-16 23:04:24,961] Trial 47 finished with value: 0.07131721079349518 and parameters: {'seq_len': 48, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.000857940067299971, 'dropout': 0.03424471217924429}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  94%|█████████▍| 47/50 [10:18<00:52, 17.39s/it]Best trial: 20. Best value: 0.0703416:  94%|█████████▍| 47/50 [10:19<00:52, 17.39s/it]Best trial: 20. Best value: 0.0703416:  96%|█████████▌| 48/50 [10:19<00:35, 17.90s/it]
============================================================
Trial 48
============================================================
seq_len: 48, pred_len: 14
d_model: 64, n_heads: 8
e_layers: 1, d_layers: 1
batch_size: 16, lr: 0.000804
dropout: 0.2564936909139591
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_48 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.040184).  Saving model ...
Updating learning rate to 0.0008039682567263566
Validation loss decreased (0.040184 --> 0.039752).  Saving model ...
Updating learning rate to 0.0004019841283631783
Validation loss decreased (0.039752 --> 0.039153).  Saving model ...
Updating learning rate to 0.00020099206418158915
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010049603209079458
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.024801604539729e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_48 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06603606790304184, mae:0.10106465220451355, dtw:Not calculated


                                                                                      [I 2025-10-16 23:04:41,205] Trial 48 finished with value: 0.10106465220451355 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 1, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0008039682567263566, 'dropout': 0.2564936909139591}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  96%|█████████▌| 48/50 [10:35<00:35, 17.90s/it]Best trial: 20. Best value: 0.0703416:  96%|█████████▌| 48/50 [10:35<00:35, 17.90s/it]Best trial: 20. Best value: 0.0703416:  98%|█████████▊| 49/50 [10:35<00:17, 17.40s/it]
============================================================
Trial 49
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000984
dropout: 0.02174137972517412
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_49 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.026388).  Saving model ...
Updating learning rate to 0.0009843443828925539
Validation loss decreased (0.026388 --> 0.025928).  Saving model ...
Updating learning rate to 0.0004921721914462769
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024608609572313847
Validation loss decreased (0.025928 --> 0.024959).  Saving model ...
Updating learning rate to 0.00012304304786156923
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.152152393078462e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.076076196539231e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_49 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.04987803474068642, mae:0.07244950532913208, dtw:Not calculated


                                                                                      [I 2025-10-16 23:05:00,141] Trial 49 finished with value: 0.07244950532913208 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 2, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0009843443828925539, 'dropout': 0.02174137972517412}. Best is trial 20 with value: 0.070341557264328.
Best trial: 20. Best value: 0.0703416:  98%|█████████▊| 49/50 [10:54<00:17, 17.40s/it]Best trial: 20. Best value: 0.0703416:  98%|█████████▊| 49/50 [10:54<00:17, 17.40s/it]Best trial: 20. Best value: 0.0703416: 100%|██████████| 50/50 [10:54<00:00, 17.86s/it]Best trial: 20. Best value: 0.0703416: 100%|██████████| 50/50 [10:54<00:00, 13.08s/it]

======================================================================
Optimization Complete!
======================================================================
Number of finished trials: 50

Best trial:
  Value (MSE): 0.070342

Best hyperparameters:
  seq_len: 48
  pred_len: 1
  expand: 2
  d_model: 64
  n_heads: 8
  e_layers: 2
  d_layers: 1
  batch_size: 32
  learning_rate: 0.0005507127388623045
  dropout: 0.05561617345359868
======================================================================

✅ Best parameters saved to: ./optuna_results/Mamba_Exchange_14_best_params.json
✅ Visualizations saved to ./optuna_results/

======================================================================
✅ Hyperparameter tuning complete!
✅ Best parameters saved to: ./optuna_results/Mamba_Exchange_14_best_params.json
======================================================================

Next steps:
  1. Review visualizations in optuna_results/ folder
  2. Train final model:
     python train_best_model.py --model Mamba --train_epochs 100
======================================================================
[I 2025-10-16 23:05:14,218] A new study created in RDB with name: Mamba_Exchange_30

======================================================================
Starting Optuna Hyperparameter Optimization
======================================================================
Model: Mamba
Dataset: custom
Number of trials: 50
Study name: Mamba_Exchange_30
Storage: sqlite:///optuna_study.db
======================================================================

  0%|          | 0/50 [00:00<?, ?it/s]
============================================================
Trial 0
============================================================
seq_len: 96, pred_len: 1
d_model: 64, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000500
dropout: 0.1160101757941579
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_0 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.030066).  Saving model ...
Updating learning rate to 0.0005004992148429017
Validation loss decreased (0.030066 --> 0.025477).  Saving model ...
Updating learning rate to 0.00025024960742145083
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012512480371072542
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.256240185536271e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_0 >>>>>>
test 261
test shape: (261, 1, 5) (261, 1, 5)
test shape: (261, 1, 5) (261, 1, 5)


	mse:0.05125703290104866, mae:0.07709306478500366, dtw:Not calculated


                                      [I 2025-10-16 23:05:30,289] Trial 0 finished with value: 0.07709306478500366 and parameters: {'seq_len': 96, 'pred_len': 1, 'expand': 1, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 0.0005004992148429017, 'dropout': 0.1160101757941579}. Best is trial 0 with value: 0.07709306478500366.
  0%|          | 0/50 [00:16<?, ?it/s]Best trial: 0. Best value: 0.0770931:   0%|          | 0/50 [00:16<?, ?it/s]Best trial: 0. Best value: 0.0770931:   2%|▏         | 1/50 [00:16<13:08, 16.09s/it]
============================================================
Trial 1
============================================================
seq_len: 48, pred_len: 7
d_model: 32, n_heads: 8
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000057
dropout: 0.1981237942018867
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_1 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 7
============================================================
train 4619
val 253
test 255
Validation loss decreased (inf --> 0.036943).  Saving model ...
Updating learning rate to 5.704069544697648e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.852034772348824e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.426017386174412e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_1 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06415814906358719, mae:0.09791720658540726, dtw:Not calculated


                                                                                    [I 2025-10-16 23:05:35,952] Trial 1 finished with value: 0.09791720658540726 and parameters: {'seq_len': 48, 'pred_len': 7, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 5.704069544697648e-05, 'dropout': 0.1981237942018867}. Best is trial 0 with value: 0.07709306478500366.
Best trial: 0. Best value: 0.0770931:   2%|▏         | 1/50 [00:21<13:08, 16.09s/it]Best trial: 0. Best value: 0.0770931:   2%|▏         | 1/50 [00:21<13:08, 16.09s/it]Best trial: 0. Best value: 0.0770931:   4%|▍         | 2/50 [00:21<07:57,  9.96s/it]
============================================================
Trial 2
============================================================
seq_len: 48, pred_len: 30
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 1
batch_size: 64, lr: 0.000649
dropout: 0.2039718892284837
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_2 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 30
============================================================
train 4596
val 230
test 232
Validation loss decreased (inf --> 0.037754).  Saving model ...
Updating learning rate to 0.0006488180385387186
Validation loss decreased (0.037754 --> 0.036444).  Saving model ...
Updating learning rate to 0.0003244090192693593
Validation loss decreased (0.036444 --> 0.036169).  Saving model ...
Updating learning rate to 0.00016220450963467964
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.110225481733982e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.055112740866991e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_2 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.07190756499767303, mae:0.1086415946483612, dtw:Not calculated


                                                                                    [I 2025-10-16 23:05:40,426] Trial 2 finished with value: 0.1086415946483612 and parameters: {'seq_len': 48, 'pred_len': 30, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 1, 'batch_size': 64, 'learning_rate': 0.0006488180385387186, 'dropout': 0.2039718892284837}. Best is trial 0 with value: 0.07709306478500366.
Best trial: 0. Best value: 0.0770931:   4%|▍         | 2/50 [00:26<07:57,  9.96s/it]Best trial: 0. Best value: 0.0770931:   4%|▍         | 2/50 [00:26<07:57,  9.96s/it]Best trial: 0. Best value: 0.0770931:   6%|▌         | 3/50 [00:26<05:50,  7.45s/it]
============================================================
Trial 3
============================================================
seq_len: 96, pred_len: 14
d_model: 64, n_heads: 4
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000443
dropout: 0.21911993885653122
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 128, expand: 2
>>>>>> Start training: trial_3 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 14
============================================================
train 4564
val 246
test 248
Validation loss decreased (inf --> 0.038991).  Saving model ...
Updating learning rate to 0.0004430920307990789
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022154601539953945
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011077300769976973
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_3 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06645675003528595, mae:0.10455233603715897, dtw:Not calculated


                                                                                    [I 2025-10-16 23:05:51,308] Trial 3 finished with value: 0.10455233603715897 and parameters: {'seq_len': 96, 'pred_len': 14, 'expand': 2, 'd_model': 64, 'n_heads': 4, 'e_layers': 2, 'd_layers': 2, 'batch_size': 16, 'learning_rate': 0.0004430920307990789, 'dropout': 0.21911993885653122}. Best is trial 0 with value: 0.07709306478500366.
Best trial: 0. Best value: 0.0770931:   6%|▌         | 3/50 [00:37<05:50,  7.45s/it]Best trial: 0. Best value: 0.0770931:   6%|▌         | 3/50 [00:37<05:50,  7.45s/it]Best trial: 0. Best value: 0.0770931:   8%|▊         | 4/50 [00:37<06:45,  8.81s/it]
============================================================
Trial 4
============================================================
seq_len: 96, pred_len: 7
d_model: 64, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 64, lr: 0.000058
dropout: 0.1028985121117678
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 64, d_state: 16, d_inner: 64, expand: 1
>>>>>> Start training: trial_4 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 7
============================================================
train 4571
val 253
test 255
Validation loss decreased (inf --> 0.037115).  Saving model ...
Updating learning rate to 5.76194710100549e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.880973550502745e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.4404867752513724e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_4 >>>>>>
test 255
test shape: (255, 7, 5) (255, 7, 5)
test shape: (255, 7, 5) (255, 7, 5)


	mse:0.06635331362485886, mae:0.11501414328813553, dtw:Not calculated


                                                                                    [I 2025-10-16 23:05:54,452] Trial 4 finished with value: 0.11501414328813553 and parameters: {'seq_len': 96, 'pred_len': 7, 'expand': 1, 'd_model': 64, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 64, 'learning_rate': 5.76194710100549e-05, 'dropout': 0.1028985121117678}. Best is trial 0 with value: 0.07709306478500366.
Best trial: 0. Best value: 0.0770931:   8%|▊         | 4/50 [00:40<06:45,  8.81s/it]Best trial: 0. Best value: 0.0770931:   8%|▊         | 4/50 [00:40<06:45,  8.81s/it]Best trial: 0. Best value: 0.0770931:  10%|█         | 5/50 [00:40<05:04,  6.77s/it]
============================================================
Trial 5
============================================================
seq_len: 48, pred_len: 14
d_model: 32, n_heads: 4
e_layers: 1, d_layers: 2
batch_size: 32, lr: 0.000091
dropout: 0.015491175876075569
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_5 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 48, Prediction length: 14
============================================================
train 4612
val 246
test 248
Validation loss decreased (inf --> 0.038259).  Saving model ...
Updating learning rate to 9.054606966770751e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.5273034833853756e-05
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.2636517416926878e-05
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_5 >>>>>>
test 248
test shape: (248, 14, 5) (248, 14, 5)
test shape: (248, 14, 5) (248, 14, 5)


	mse:0.06361096352338791, mae:0.09909265488386154, dtw:Not calculated


                                                                                    [I 2025-10-16 23:06:00,260] Trial 5 finished with value: 0.09909265488386154 and parameters: {'seq_len': 48, 'pred_len': 14, 'expand': 2, 'd_model': 32, 'n_heads': 4, 'e_layers': 1, 'd_layers': 2, 'batch_size': 32, 'learning_rate': 9.054606966770751e-05, 'dropout': 0.015491175876075569}. Best is trial 0 with value: 0.07709306478500366.
Best trial: 0. Best value: 0.0770931:  10%|█         | 5/50 [00:46<05:04,  6.77s/it]Best trial: 0. Best value: 0.0770931:  10%|█         | 5/50 [00:46<05:04,  6.77s/it]Best trial: 0. Best value: 0.0770931:  12%|█▏        | 6/50 [00:46<04:43,  6.44s/it]
============================================================
Trial 6
============================================================
seq_len: 96, pred_len: 30
d_model: 32, n_heads: 8
e_layers: 2, d_layers: 1
batch_size: 16, lr: 0.000019
dropout: 0.12095934455719404
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 32, expand: 1
>>>>>> Start training: trial_6 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 30
============================================================
train 4548
val 230
test 232
Validation loss decreased (inf --> 0.037034).  Saving model ...
Updating learning rate to 1.906351354176898e-05
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.53175677088449e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.765878385442245e-06
Validation loss decreased (0.037034 --> 0.036612).  Saving model ...
Updating learning rate to 2.3829391927211223e-06
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.1914695963605612e-06
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.957347981802806e-07
EarlyStopping counter: 3 out of 3
>>>>>> Testing on validation set: trial_6 >>>>>>
test 232
test shape: (232, 30, 5) (232, 30, 5)
test shape: (232, 30, 5) (232, 30, 5)


	mse:0.06912851333618164, mae:0.12122957408428192, dtw:Not calculated


                                                                                    [I 2025-10-16 23:06:18,526] Trial 6 finished with value: 0.12122957408428192 and parameters: {'seq_len': 96, 'pred_len': 30, 'expand': 1, 'd_model': 32, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'batch_size': 16, 'learning_rate': 1.906351354176898e-05, 'dropout': 0.12095934455719404}. Best is trial 0 with value: 0.07709306478500366.
Best trial: 0. Best value: 0.0770931:  12%|█▏        | 6/50 [01:04<04:43,  6.44s/it]Best trial: 0. Best value: 0.0770931:  12%|█▏        | 6/50 [01:04<04:43,  6.44s/it]Best trial: 0. Best value: 0.0770931:  14%|█▍        | 7/50 [01:04<07:23, 10.31s/it]
============================================================
Trial 7
============================================================
seq_len: 96, pred_len: 1
d_model: 32, n_heads: 4
e_layers: 2, d_layers: 2
batch_size: 16, lr: 0.000212
dropout: 0.27622242387170737
============================================================

Please make sure you have successfully installed mamba_ssm
Use GPU: cuda:0
DEBUG Mamba Init - d_model: 32, d_state: 16, d_inner: 64, expand: 2
>>>>>> Start training: trial_7 >>>>>>
============================================================
DATASET SPLIT INFO (90/5/5):
============================================================
Total samples: 5193
Train: 4673 samples (90%) - rows 0 to 4672
Val: 259 samples (5%) - rows 4673 to 4931
Test: 261 samples (5%) - rows 4932 to 5192
Sequence length: 96, Prediction length: 1
============================================================
train 4577
val 259
test 261
Validation loss decreased (inf --> 0.031374).  Saving model ...
Updating learning rate to 0.000212230198735043
Validation loss decreased (0.031374 --> 0.028341).  Saving model ...
Updating learning rate to 0.0001061150993675215
